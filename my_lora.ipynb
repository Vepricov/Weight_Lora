{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# To crate venv use \n",
    "# python -m venv `/path/to/new/virtual/environment`\n",
    "import torch\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets \n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-SXM4-80GB\n",
      "GPU available? True\n"
     ]
    }
   ],
   "source": [
    "# прикол для сервера\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\n",
    "\n",
    "def set_seed(seed): # ставит сид\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def set_device(device_no: int): # выбирает GPU-шку и выводит название\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f\"cuda:{device_no}\")\n",
    "        print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "        print(\"We will use the GPU:\", torch.cuda.get_device_name(device_no))\n",
    "    else:\n",
    "        print(\"No GPU available, using the CPU instead.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "set_seed(18)\n",
    "device = set_device(2)\n",
    "print(f'GPU available? {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n",
      "model device = cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "weight_type = torch.float16 # in gpu can be torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=weight_type,\n",
    "    load_in_8bit=False,\n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(model)\n",
    "device = model.device\n",
    "print(f'model device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# class CastOutputToFloat(nn.Sequential):\n",
    "#     def forward(self, x): return super().forward(x).to(weight_type)\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen layers: \n",
      "#1:  name = roberta.encoder.layer.0.attention.self.query\n",
      "#2:  name = roberta.encoder.layer.0.attention.self.value\n",
      "#3:  name = roberta.encoder.layer.1.attention.self.query\n",
      "#4:  name = roberta.encoder.layer.1.attention.self.value\n",
      "#5:  name = roberta.encoder.layer.2.attention.self.query\n",
      "#6:  name = roberta.encoder.layer.2.attention.self.value\n",
      "#7:  name = roberta.encoder.layer.3.attention.self.query\n",
      "#8:  name = roberta.encoder.layer.3.attention.self.value\n",
      "#9:  name = roberta.encoder.layer.4.attention.self.query\n",
      "#10:  name = roberta.encoder.layer.4.attention.self.value\n",
      "#11:  name = roberta.encoder.layer.5.attention.self.query\n",
      "#12:  name = roberta.encoder.layer.5.attention.self.value\n",
      "#13:  name = roberta.encoder.layer.6.attention.self.query\n",
      "#14:  name = roberta.encoder.layer.6.attention.self.value\n",
      "#15:  name = roberta.encoder.layer.7.attention.self.query\n",
      "#16:  name = roberta.encoder.layer.7.attention.self.value\n",
      "#17:  name = roberta.encoder.layer.8.attention.self.query\n",
      "#18:  name = roberta.encoder.layer.8.attention.self.value\n",
      "#19:  name = roberta.encoder.layer.9.attention.self.query\n",
      "#20:  name = roberta.encoder.layer.9.attention.self.value\n",
      "#21:  name = roberta.encoder.layer.10.attention.self.query\n",
      "#22:  name = roberta.encoder.layer.10.attention.self.value\n",
      "#23:  name = roberta.encoder.layer.11.attention.self.query\n",
      "#24:  name = roberta.encoder.layer.11.attention.self.value\n",
      "Overall chosen layers: 24\n"
     ]
    }
   ],
   "source": [
    "layer_filter = lambda name : ('attention' in name) and \\\n",
    "    (('query' in name) or ('value' in name)) and \\\n",
    "    ('bias' not in name)\n",
    "    \n",
    "print(f\"Chosen layers: \")\n",
    "chosen_layers = [] \n",
    "for name, param in model.named_modules():\n",
    "    if layer_filter(name):\n",
    "        right_name = name\n",
    "        if model_name == \"FacebookAI/roberta-base\":\n",
    "            chosen_layers.append(name)\n",
    "        print(f\"#{len(chosen_layers)}:  name = {name}\")\n",
    "\n",
    "print(f\"Overall chosen layers: {len(chosen_layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_layers = chosen_layers[:2] # baby steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "for name, param in model.named_parameters():\n",
    "    # if name not in chosen_layers:\n",
    "    #     param.requires_grad = False\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32) \n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 124697433 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pipelines.utils as utils\n",
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adapter\n",
    "for i in range(0, len(chosen_layers) // 2):\n",
    "    model.roberta.encoder.layer[i].attention.self.query = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.query)\n",
    "    model.roberta.encoder.layer[i].attention.self.value = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1179648 || all params: 125877081 || trainable%: 0.9371427988547018\n"
     ]
    }
   ],
   "source": [
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForCausalLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): AdapterLayer(\n",
       "                (module): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (adapter): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): AdapterLayer(\n",
       "                (module): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (adapter): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# sentence = \"I enjoy to <mask> in sberbank.\"\n",
    "# unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer, device=device)\n",
    "# print(f\"Original sentence: {sentence}\")\n",
    "# predictions = unmasker(sentence)\n",
    "# for i in range(len(predictions)):\n",
    "#     print(f\"#{i+1}: {predictions[i]['sequence']} || score = {predictions[i]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 53.2k/53.2k [00:00<00:00, 483kB/s]\n",
      "Downloading metadata: 100%|██████████| 138k/138k [00:00<00:00, 3.85MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'cais/mmlu'\n",
    "dataset_config_name = 'philosophy'\n",
    "\n",
    "# Can be changed to wiki (as in Micrisoft):\n",
    "# dataset_name = 'wikitext'\n",
    "# dataset_config_name = 'wikitext-2-raw-v1'\n",
    "\n",
    "dataset = load_dataset(dataset_name, dataset_config_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____.\n",
      "subject: philosophy\n",
      "choices: ['the state', 'the justice system', 'the body', 'the soul']\n",
      "answer: 3\n"
     ]
    }
   ],
   "source": [
    "# only for MMLU\n",
    "num = 1\n",
    "print(f\"question: {dataset['test']['question'][num]}\")\n",
    "print(f\"subject: {dataset['test']['subject'][num]}\")\n",
    "print(f\"choices: {dataset['test']['choices'][num]}\")\n",
    "print(f\"answer: {dataset['test']['answer'][num]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "def make_mlm_dataset_form_mmlu(dataset, head=3):\n",
    "    dataset_list = []\n",
    "    for a in dataset:\n",
    "        q = a['question']\n",
    "        q = q.replace('_', '')\n",
    "        q += ' ' + a['choices'][a['answer']]\n",
    "        q = q.replace('.', '')\n",
    "        q = q.replace('  ', ' ')\n",
    "        q += '.'\n",
    "        dataset_list.append({\"text\" : q})\n",
    "\n",
    "    if head > 0:\n",
    "        print(\"Examples:\")\n",
    "    for i, a in enumerate(dataset_list[:head]):\n",
    "        print(f\"#{i+1}: {a['text']}\")\n",
    "\n",
    "    return_dataset = datasets.Dataset.from_list(dataset_list)\n",
    "    return return_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Examples:\n",
      "#1: Aesthetics deals with objects that are not essential to our existence.\n",
      "#2: For Socrates, an unexamined life is a tragedy because it results in grievous harm to the soul.\n",
      "#3: According to Kant, nothing can be called “good” without qualification except a good will.\n",
      "NUM ROWS = 311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST\n",
      "Examples:\n",
      "#1: One of the aims of philosophy is to think critically about whether there are good reasons for adopting our beliefs Reasons are considered \"good reasons\" if they are consistent with everyday experience and: take into account objections, are acceptable to impartial third parties, and avoid undesirable consequences.\n",
      "#2: The existence of a form of mental illness known as multiple personality disorder seems to suggest that the mind is divisible.\n",
      "#3: Singer’s argument begins with the assumption that: suffering and death from lack of food, shelter, and medical care are bad.\n",
      "NUM ROWS = 34\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VALIDATION\n",
      "Examples:\n",
      "#1: Psychological egoism is: a claim about human nature and the ways people are capable of behaving.\n",
      "#2: According to Moore’s “ideal utilitarianism,” the right action is the one that brings about the greatest amount of: good.\n",
      "#3: According to d'Holbach, people always act according to necessary natural laws.\n",
      "NUM ROWS = 5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TRAIN')\n",
    "train = make_mlm_dataset_form_mmlu(dataset['test'])\n",
    "print(f'NUM ROWS = {len(train)}', '-'*100, sep='\\n')\n",
    "print('TEST')\n",
    "test = make_mlm_dataset_form_mmlu(dataset['validation'])\n",
    "print(f'NUM ROWS = {len(test)}', '-'*100, sep='\\n')\n",
    "print('VALIDATION')\n",
    "val = make_mlm_dataset_form_mmlu(dataset['dev'])\n",
    "print(f'NUM ROWS = {len(val)}', '-'*100, sep='\\n')\n",
    "dataset = datasets.DatasetDict({\"test\" : test,\n",
    "                                \"train\" : train,\n",
    "                                \"validation\" : val})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_roberta.RobertaForCausalLM"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34/34 [00:00<00:00, 1604.34 examples/s]\n",
      "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 311/311 [00:00<00:00, 11654.80 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1354.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshkodnik\u001b[0m (\u001b[33mshkodnik-mipt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"SBER_LORA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14155776 || all params: 138853209 || trainable%: 10.194777709458627\n"
     ]
    }
   ],
   "source": [
    "# reload model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=weight_type,\n",
    "    load_in_8bit=False,\n",
    "    device_map=device\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float32) \n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i in range(0, len(chosen_layers) // 2):\n",
    "    model.roberta.encoder.layer[i].attention.self.query = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.query)\n",
    "    model.roberta.encoder.layer[i].attention.self.value = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.value)\n",
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pipelines.utils' has no attribute 'approx_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m : model_name,\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoIHT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m1.\u001b[39m, \u001b[38;5;66;03m# c какой вероятностью делать approx\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox_func\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapprox_0\u001b[49m,\n\u001b[1;32m      6\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproj_func\u001b[39m\u001b[38;5;124m\"\u001b[39m : utils\u001b[38;5;241m.\u001b[39mproj_0,\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      8\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m100\u001b[39m}\n\u001b[1;32m      9\u001b[0m report_to \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# \"none\" or \"wandb\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pipelines.utils' has no attribute 'approx_0'"
     ]
    }
   ],
   "source": [
    "wandb_config = {\"model_name\" : model_name,\n",
    "                \"optimizer\" : \"StoIHT\",\n",
    "                \"prob\" : 1., # c какой вероятностью делать approx\n",
    "                \"k\" : 50,\n",
    "                \"approx_func\" : utils.approx_0,\n",
    "                \"proj_func\" : utils.proj_0,\n",
    "                \"lr\" : 0.1,\n",
    "                \"max_steps\" : 100}\n",
    "report_to = \"none\" # \"none\" or \"wandb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mwandb_config\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoIHT\u001b[39m\u001b[38;5;124m\"\u001b[39m:    \n\u001b[1;32m      2\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mStoIHT(model\u001b[38;5;241m.\u001b[39mparameters(), k\u001b[38;5;241m=\u001b[39mwandb_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      3\u001b[0m                              approx\u001b[38;5;241m=\u001b[39mwandb_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox_func\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      4\u001b[0m                              proj\u001b[38;5;241m=\u001b[39mwandb_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproj_func\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      5\u001b[0m                              lr\u001b[38;5;241m=\u001b[39mwandb_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      6\u001b[0m                              prob\u001b[38;5;241m=\u001b[39mwandb_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb_config' is not defined"
     ]
    }
   ],
   "source": [
    "if wandb_config[\"optimizer\"] == \"StoIHT\":    \n",
    "    optimizer = utils.StoIHT(model.parameters(), k=wandb_config[\"k\"], \n",
    "                             approx=wandb_config[\"approx_func\"], \n",
    "                             proj=wandb_config[\"proj_func\"], \n",
    "                             lr=wandb_config[\"lr\"], \n",
    "                             prob=wandb_config[\"prob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import time\n",
    "Time = str(time.ctime()).replace(\"  \", \" \").replace(\" \", \"_\").replace(':', '-')\n",
    "\n",
    "ars = transformers.TrainingArguments(\n",
    "    # per_device_train_batch_size=1, \n",
    "    # gradient_accumulation_steps=16, \n",
    "    # warmup_steps=10, \n",
    "    # max_steps=wandb_config[\"max_steps\"], \n",
    "    max_steps=10,\n",
    "    # learning_rate=1e-3, \n",
    "    fp16=True, \n",
    "    output_dir=f\"my_lora/outputs/{Time}\", \n",
    "    use_cpu=False, \n",
    "    save_safetensors=False,\n",
    "    # report_to=report_to,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=1,\n",
    "    # run_name=f\"prob={int(prob*100)}/100_k={k}\",\n",
    "    run_name=f\"{Time}\",\n",
    "    # run_name=\"test\"\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    args=ars,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15),\n",
    "    optimizers=[None, None],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>16.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>16.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>16.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>15.661200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ret = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shkodnik/Sber_Lora/VeprVenv/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:11, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.990900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>8.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>9.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.060200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.473300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>7.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>8.612700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>6.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>8.094500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>6.805100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>7.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>7.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>7.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>8.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>7.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>7.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>6.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>6.502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>7.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>7.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>8.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>6.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>7.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>7.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>7.400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>7.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>7.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>7.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>8.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>6.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>8.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>7.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.539400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>7.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>7.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>7.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>7.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>6.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>7.456600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>7.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>7.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>8.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.853900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>7.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>7.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>6.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>6.972400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>6.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>7.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>7.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>7.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>6.785200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>8.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>7.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>8.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>6.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>6.871700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>7.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>7.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>7.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>7.581200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>6.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.257600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>6.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>7.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>7.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>7.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>6.739300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>7.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>7.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>9.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>6.800600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>8.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>7.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>6.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>6.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>7.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>7.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>7.648300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>6.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>8.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.186000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ret = trainer.train()\n",
    "    if report_to == \"wandb\":\n",
    "        wandb.config.update(wandb_config)\n",
    "        wandb.finish()\n",
    "except Exception as err:\n",
    "    print(\"ERROR!\")\n",
    "    print(err)\n",
    "    if report_to == \"wandb\":\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a25179e0c4453f8cd6d360eddc955e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 18.2847900390625,\n",
       " 'eval_runtime': 1.8194,\n",
       " 'eval_samples_per_second': 18.687,\n",
       " 'eval_steps_per_second': 2.748,\n",
       " 'epoch': 0.05128205128205128}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
