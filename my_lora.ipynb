{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU available? True\n"
     ]
    }
   ],
   "source": [
    "# прикол для сервера\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\n",
    "\n",
    "def set_seed(seed): # ставит сид\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def set_device(device_no: int): # выбирает GPU-шку и выводит название\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f\"cuda:{device_no}\")\n",
    "        print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "        print(\"We will use the GPU:\", torch.cuda.get_device_name(device_no))\n",
    "    else:\n",
    "        print(\"No GPU available, using the CPU instead.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "set_seed(18)\n",
    "device = set_device(7)\n",
    "print(f'GPU available? {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreisemenov/Veprikov/Sber_Lora/VeprVenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n",
      "model device = cuda:7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreisemenov/Veprikov/Sber_Lora/VeprVenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "weight_type = torch.float16 # in gpu can be torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=weight_type,\n",
    "    load_in_8bit=False,\n",
    "    device_map=device,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(model)\n",
    "device = model.device\n",
    "print(f'model device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# class CastOutputToFloat(nn.Sequential):\n",
    "#     def forward(self, x): return super().forward(x).to(weight_type)\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen layers: \n",
      "#1:  name = roberta.encoder.layer.0.attention.self.query\n",
      "#2:  name = roberta.encoder.layer.0.attention.self.value\n",
      "#3:  name = roberta.encoder.layer.1.attention.self.query\n",
      "#4:  name = roberta.encoder.layer.1.attention.self.value\n",
      "#5:  name = roberta.encoder.layer.2.attention.self.query\n",
      "#6:  name = roberta.encoder.layer.2.attention.self.value\n",
      "#7:  name = roberta.encoder.layer.3.attention.self.query\n",
      "#8:  name = roberta.encoder.layer.3.attention.self.value\n",
      "#9:  name = roberta.encoder.layer.4.attention.self.query\n",
      "#10:  name = roberta.encoder.layer.4.attention.self.value\n",
      "#11:  name = roberta.encoder.layer.5.attention.self.query\n",
      "#12:  name = roberta.encoder.layer.5.attention.self.value\n",
      "#13:  name = roberta.encoder.layer.6.attention.self.query\n",
      "#14:  name = roberta.encoder.layer.6.attention.self.value\n",
      "#15:  name = roberta.encoder.layer.7.attention.self.query\n",
      "#16:  name = roberta.encoder.layer.7.attention.self.value\n",
      "#17:  name = roberta.encoder.layer.8.attention.self.query\n",
      "#18:  name = roberta.encoder.layer.8.attention.self.value\n",
      "#19:  name = roberta.encoder.layer.9.attention.self.query\n",
      "#20:  name = roberta.encoder.layer.9.attention.self.value\n",
      "#21:  name = roberta.encoder.layer.10.attention.self.query\n",
      "#22:  name = roberta.encoder.layer.10.attention.self.value\n",
      "#23:  name = roberta.encoder.layer.11.attention.self.query\n",
      "#24:  name = roberta.encoder.layer.11.attention.self.value\n",
      "Overall chosen layers: 24\n"
     ]
    }
   ],
   "source": [
    "layer_filter = lambda name : ('attention' in name) and \\\n",
    "    (('query' in name) or ('value' in name)) and \\\n",
    "    ('bias' not in name)\n",
    "    \n",
    "print(f\"Chosen layers: \")\n",
    "chosen_layers = []\n",
    "for name, param in model.named_modules():\n",
    "    if layer_filter(name):\n",
    "        right_name = name\n",
    "        if model_name == \"FacebookAI/roberta-base\":\n",
    "            chosen_layers.append(name)\n",
    "        print(f\"#{len(chosen_layers)}:  name = {name}\")\n",
    "\n",
    "print(f\"Overall chosen layers: {len(chosen_layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_layers = chosen_layers[:2] # baby steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "for name, param in model.named_parameters():\n",
    "    # if name not in chosen_layers:\n",
    "    #     param.requires_grad = False\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32) \n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 124697433 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adapter\n",
    "for i in range(0, len(chosen_layers) // 2):\n",
    "    model.roberta.encoder.layer[i].attention.self.query = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.query)\n",
    "    model.roberta.encoder.layer[i].attention.self.value = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14155776 || all params: 138853209 || trainable%: 10.194777709458627\n"
     ]
    }
   ],
   "source": [
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# sentence = \"I enjoy to <mask> in sberbank.\"\n",
    "# unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer, device=device)\n",
    "# print(f\"Original sentence: {sentence}\")\n",
    "# predictions = unmasker(sentence)\n",
    "# for i in range(len(predictions)):\n",
    "#     print(f\"#{i+1}: {predictions[i]['sequence']} || score = {predictions[i]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'cais/mmlu'\n",
    "dataset_config_name = 'philosophy'\n",
    "\n",
    "# Can be changed to wiki (as in Micrisoft):\n",
    "# dataset_name = 'wikitext'\n",
    "# dataset_config_name = 'wikitext-2-raw-v1'\n",
    "\n",
    "dataset = load_dataset(dataset_name, dataset_config_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____.\n",
      "subject: philosophy\n",
      "choices: ['the state', 'the justice system', 'the body', 'the soul']\n",
      "answer: 3\n"
     ]
    }
   ],
   "source": [
    "# only for MMLU\n",
    "num = 1\n",
    "print(f\"question: {dataset['test']['question'][num]}\")\n",
    "print(f\"subject: {dataset['test']['subject'][num]}\")\n",
    "print(f\"choices: {dataset['test']['choices'][num]}\")\n",
    "print(f\"answer: {dataset['test']['answer'][num]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "def make_mlm_dataset_form_mmlu(dataset, head=3):\n",
    "    dataset_list = []\n",
    "    for a in dataset:\n",
    "        q = a['question']\n",
    "        q = q.replace('_', '')\n",
    "        q += ' ' + a['choices'][a['answer']]\n",
    "        q = q.replace('.', '')\n",
    "        q = q.replace('  ', ' ')\n",
    "        q += '.'\n",
    "        dataset_list.append({\"text\" : q})\n",
    "\n",
    "    if head > 0:\n",
    "        print(\"Examples:\")\n",
    "    for i, a in enumerate(dataset_list[:head]):\n",
    "        print(f\"#{i+1}: {a['text']}\")\n",
    "\n",
    "    return_dataset = datasets.Dataset.from_list(dataset_list)\n",
    "    return return_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Examples:\n",
      "#1: Aesthetics deals with objects that are not essential to our existence.\n",
      "#2: For Socrates, an unexamined life is a tragedy because it results in grievous harm to the soul.\n",
      "#3: According to Kant, nothing can be called “good” without qualification except a good will.\n",
      "NUM ROWS = 311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST\n",
      "Examples:\n",
      "#1: One of the aims of philosophy is to think critically about whether there are good reasons for adopting our beliefs Reasons are considered \"good reasons\" if they are consistent with everyday experience and: take into account objections, are acceptable to impartial third parties, and avoid undesirable consequences.\n",
      "#2: The existence of a form of mental illness known as multiple personality disorder seems to suggest that the mind is divisible.\n",
      "#3: Singer’s argument begins with the assumption that: suffering and death from lack of food, shelter, and medical care are bad.\n",
      "NUM ROWS = 34\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VALIDATION\n",
      "Examples:\n",
      "#1: Psychological egoism is: a claim about human nature and the ways people are capable of behaving.\n",
      "#2: According to Moore’s “ideal utilitarianism,” the right action is the one that brings about the greatest amount of: good.\n",
      "#3: According to d'Holbach, people always act according to necessary natural laws.\n",
      "NUM ROWS = 5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TRAIN')\n",
    "train = make_mlm_dataset_form_mmlu(dataset['test'])\n",
    "print(f'NUM ROWS = {len(train)}', '-'*100, sep='\\n')\n",
    "print('TEST')\n",
    "test = make_mlm_dataset_form_mmlu(dataset['validation'])\n",
    "print(f'NUM ROWS = {len(test)}', '-'*100, sep='\\n')\n",
    "print('VALIDATION')\n",
    "val = make_mlm_dataset_form_mmlu(dataset['dev'])\n",
    "print(f'NUM ROWS = {len(val)}', '-'*100, sep='\\n')\n",
    "dataset = datasets.DatasetDict({\"test\" : test,\n",
    "                                \"train\" : train,\n",
    "                                \"validation\" : val})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34/34 [00:00<00:00, 2421.61 examples/s]\n",
      "Map: 100%|██████████| 311/311 [00:00<00:00, 18085.66 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1145.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msolodkin-vs\u001b[0m (\u001b[33mshkodnik-mipt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"SBER_LORA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14155776 || all params: 138853209 || trainable%: 10.194777709458627\n"
     ]
    }
   ],
   "source": [
    "# reload model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=weight_type,\n",
    "    load_in_8bit=False,\n",
    "    device_map=device\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float32) \n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i in range(0, len(chosen_layers) // 2):\n",
    "    model.roberta.encoder.layer[i].attention.self.query = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.query)\n",
    "    model.roberta.encoder.layer[i].attention.self.value = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.value)\n",
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = {\"optimizer\" : \"StoIHT\",\n",
    "                \"prob\" : 1., # c какой вероятностью делать approx\n",
    "                \"k\" : 50,\n",
    "                \"approx_func\" : \"approx_0\",\n",
    "                \"proj_func\" : \"proj_0\",\n",
    "                \"lr\" : 0.1,\n",
    "                \"max_steps\" : 18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_config[\"optimizer\"] == \"StoIHT\":\n",
    "    if wandb_config[\"approx_func\"] == \"approx_0\":\n",
    "        approx = utils.approx_0\n",
    "    else:\n",
    "        raise ValueError(f'Wrong aprrox func {wandb_config[\"approx_func\"]}!')\n",
    "    if wandb_config[\"proj_func\"] == \"proj_0\":\n",
    "        proj = utils.proj_0\n",
    "    else:\n",
    "        raise ValueError(f'Wrong proj func {wandb_config[\"proj_func\"]}!')\n",
    "    \n",
    "    optimizer = utils.StoIHT(model.parameters(), k=wandb_config[\"k\"], \n",
    "                             approx=approx, proj=proj, lr=1e-1, \n",
    "                             prob=wandb_config[\"prob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreisemenov/Veprikov/Sber_Lora/VeprVenv/lib/python3.11/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import time\n",
    "Time = str(time.ctime()).replace(\"  \", \" \").replace(\" \", \"_\").replace(':', '-')\n",
    "\n",
    "ars = transformers.TrainingArguments(\n",
    "    # per_device_train_batch_size=1, \n",
    "    # gradient_accumulation_steps=16, \n",
    "    # warmup_steps=10, \n",
    "    max_steps=wandb_config[\"max_steps\"], \n",
    "    # learning_rate=1e-3, \n",
    "    fp16=True, \n",
    "    output_dir=f\"my_lora/outputs/{Time}\", \n",
    "    use_cpu=False, \n",
    "    save_safetensors=False,\n",
    "    report_to=\"wandb\",\n",
    "    # report_to=\"none\",\n",
    "    logging_steps=1,\n",
    "    # run_name=f\"prob={int(prob*100)}/100_k={k}\",\n",
    "    # run_name=f\"{Time}\",\n",
    "    run_name=\"test\"\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    args=ars,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15),\n",
    "    optimizers=[optimizer, None],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/andreisemenov/Veprikov/Sber_Lora/wandb/run-20240819_133714-6t0r3nmp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/6t0r3nmp' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/6t0r3nmp' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/6t0r3nmp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreisemenov/Veprikov/Sber_Lora/VeprVenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>14.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>12.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>13.675400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>13.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>13.595500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>14.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15.584100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>11.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>13.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>13.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>13.107900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'wandb' has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ret \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mupdate(wandb_config)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m()\n\u001b[1;32m      4\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "ret = trainer.train()\n",
    "wandb.config.update(wandb_config)\n",
    "wandb.update()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreisemenov/Veprikov/Sber_Lora/VeprVenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.326400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.save_pretrained(output_dir, safe_serialization=False)\n",
    "try:\n",
    "    ret = trainer.train()\n",
    "    wandb.finish()\n",
    "except Exception as err:\n",
    "    print(\"ERROR!\")\n",
    "    print(err)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a25179e0c4453f8cd6d360eddc955e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 18.2847900390625,\n",
       " 'eval_runtime': 1.8194,\n",
       " 'eval_samples_per_second': 18.687,\n",
       " 'eval_steps_per_second': 2.748,\n",
       " 'epoch': 0.05128205128205128}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
