{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? False\n"
     ]
    }
   ],
   "source": [
    "# прикол для сервера\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\n",
    "\n",
    "def set_seed(seed): # ставит сид\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def set_device(device_no: int): # выбирает GPU-шку и выводит название\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f\"cuda:{device_no}\")\n",
    "        print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "        print(\"We will use the GPU:\", torch.cuda.get_device_name(device_no))\n",
    "    else:\n",
    "        print(\"No GPU available, using the CPU instead.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "set_seed(18)\n",
    "# device = set_device(7)\n",
    "print(f'GPU available? {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n",
      "model device = cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "weight_type = torch.float32 # in gpu can be torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=weight_type,\n",
    "    load_in_8bit=False,\n",
    "    device_map='cpu',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(model)\n",
    "device = model.device\n",
    "print(f'model device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# class CastOutputToFloat(nn.Sequential):\n",
    "#     def forward(self, x): return super().forward(x).to(weight_type)\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen layers: \n",
      "#1:  name = roberta.encoder.layer.0.attention.self.query\n",
      "#2:  name = roberta.encoder.layer.0.attention.self.value\n",
      "#3:  name = roberta.encoder.layer.1.attention.self.query\n",
      "#4:  name = roberta.encoder.layer.1.attention.self.value\n",
      "#5:  name = roberta.encoder.layer.2.attention.self.query\n",
      "#6:  name = roberta.encoder.layer.2.attention.self.value\n",
      "#7:  name = roberta.encoder.layer.3.attention.self.query\n",
      "#8:  name = roberta.encoder.layer.3.attention.self.value\n",
      "#9:  name = roberta.encoder.layer.4.attention.self.query\n",
      "#10:  name = roberta.encoder.layer.4.attention.self.value\n",
      "#11:  name = roberta.encoder.layer.5.attention.self.query\n",
      "#12:  name = roberta.encoder.layer.5.attention.self.value\n",
      "#13:  name = roberta.encoder.layer.6.attention.self.query\n",
      "#14:  name = roberta.encoder.layer.6.attention.self.value\n",
      "#15:  name = roberta.encoder.layer.7.attention.self.query\n",
      "#16:  name = roberta.encoder.layer.7.attention.self.value\n",
      "#17:  name = roberta.encoder.layer.8.attention.self.query\n",
      "#18:  name = roberta.encoder.layer.8.attention.self.value\n",
      "#19:  name = roberta.encoder.layer.9.attention.self.query\n",
      "#20:  name = roberta.encoder.layer.9.attention.self.value\n",
      "#21:  name = roberta.encoder.layer.10.attention.self.query\n",
      "#22:  name = roberta.encoder.layer.10.attention.self.value\n",
      "#23:  name = roberta.encoder.layer.11.attention.self.query\n",
      "#24:  name = roberta.encoder.layer.11.attention.self.value\n",
      "Overall chosen layers: 24\n"
     ]
    }
   ],
   "source": [
    "layer_filter = lambda name : ('attention' in name) and \\\n",
    "    (('query' in name) or ('value' in name)) and \\\n",
    "    ('bias' not in name)\n",
    "    \n",
    "print(f\"Chosen layers: \")\n",
    "chosen_layers = []\n",
    "for name, param in model.named_modules():\n",
    "    if layer_filter(name):\n",
    "        right_name = name\n",
    "        if model_name == \"FacebookAI/roberta-base\":\n",
    "            chosen_layers.append(name)\n",
    "        print(f\"#{len(chosen_layers)}:  name = {name}\")\n",
    "\n",
    "print(f\"Overall chosen layers: {len(chosen_layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_layers = chosen_layers[:1] # baby steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "for name, param in model.named_parameters():\n",
    "    # if name not in chosen_layers:\n",
    "    #     param.requires_grad = False\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32) \n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 124697433 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adapter\n",
    "for i in range(0, len(chosen_layers) // 2):\n",
    "    model.roberta.encoder.layer[i].attention.self.query = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.query)\n",
    "    model.roberta.encoder.layer[i].attention.self.value = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14155776 || all params: 138853209 || trainable%: 10.194777709458627\n"
     ]
    }
   ],
   "source": [
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: I enjoy to <mask> in sberbank.\n",
      "#1: I enjoy to out in sberbank. || score = 0.1738138496875763\n",
      "#2: I enjoy to� in sberbank. || score = 0.06875785440206528\n",
      "#3: I enjoy to by in sberbank. || score = 0.05697421729564667\n",
      "#4: I enjoy to support in sberbank. || score = 0.03905681148171425\n",
      "#5: I enjoy to, in sberbank. || score = 0.038994841277599335\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentence = \"I enjoy to <mask> in sberbank.\"\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "predictions = unmasker(sentence)\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"#{i+1}: {predictions[i]['sequence']} || score = {predictions[i]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'cais/mmlu'\n",
    "dataset_config_name = 'philosophy'\n",
    "\n",
    "# Can be changed to wiki (as in Micrisoft):\n",
    "# dataset_name = 'wikitext'\n",
    "# dataset_config_name = 'wikitext-2-raw-v1'\n",
    "\n",
    "dataset = load_dataset(dataset_name, dataset_config_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____.\n",
      "subject: philosophy\n",
      "choices: ['the state', 'the justice system', 'the body', 'the soul']\n",
      "answer: 3\n"
     ]
    }
   ],
   "source": [
    "# only for MMLU\n",
    "num = 1\n",
    "print(f\"question: {dataset['test']['question'][num]}\")\n",
    "print(f\"subject: {dataset['test']['subject'][num]}\")\n",
    "print(f\"choices: {dataset['test']['choices'][num]}\")\n",
    "print(f\"answer: {dataset['test']['answer'][num]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "def make_mlm_dataset_form_mmlu(dataset, head=3):\n",
    "    dataset_list = []\n",
    "    for a in dataset:\n",
    "        q = a['question']\n",
    "        q = q.replace('_', '')\n",
    "        q += ' ' + a['choices'][a['answer']]\n",
    "        q = q.replace('.', '')\n",
    "        q = q.replace('  ', ' ')\n",
    "        q += '.'\n",
    "        dataset_list.append({\"text\" : q})\n",
    "\n",
    "    if head > 0:\n",
    "        print(\"Examples:\")\n",
    "    for i, a in enumerate(dataset_list[:head]):\n",
    "        print(f\"#{i+1}: {a['text']}\")\n",
    "\n",
    "    return_dataset = datasets.Dataset.from_list(dataset_list)\n",
    "    return return_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Examples:\n",
      "#1: Aesthetics deals with objects that are not essential to our existence.\n",
      "#2: For Socrates, an unexamined life is a tragedy because it results in grievous harm to the soul.\n",
      "#3: According to Kant, nothing can be called “good” without qualification except a good will.\n",
      "NUM ROWS = 311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST\n",
      "Examples:\n",
      "#1: One of the aims of philosophy is to think critically about whether there are good reasons for adopting our beliefs Reasons are considered \"good reasons\" if they are consistent with everyday experience and: take into account objections, are acceptable to impartial third parties, and avoid undesirable consequences.\n",
      "#2: The existence of a form of mental illness known as multiple personality disorder seems to suggest that the mind is divisible.\n",
      "#3: Singer’s argument begins with the assumption that: suffering and death from lack of food, shelter, and medical care are bad.\n",
      "NUM ROWS = 34\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VALIDATION\n",
      "Examples:\n",
      "#1: Psychological egoism is: a claim about human nature and the ways people are capable of behaving.\n",
      "#2: According to Moore’s “ideal utilitarianism,” the right action is the one that brings about the greatest amount of: good.\n",
      "#3: According to d'Holbach, people always act according to necessary natural laws.\n",
      "NUM ROWS = 5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TRAIN')\n",
    "train = make_mlm_dataset_form_mmlu(dataset['test'])\n",
    "print(f'NUM ROWS = {len(train)}', '-'*100, sep='\\n')\n",
    "print('TEST')\n",
    "test = make_mlm_dataset_form_mmlu(dataset['validation'])\n",
    "print(f'NUM ROWS = {len(test)}', '-'*100, sep='\\n')\n",
    "print('VALIDATION')\n",
    "val = make_mlm_dataset_form_mmlu(dataset['dev'])\n",
    "print(f'NUM ROWS = {len(val)}', '-'*100, sep='\\n')\n",
    "dataset = datasets.DatasetDict({\"test\" : test,\n",
    "                                \"train\" : train,\n",
    "                                \"validation\" : val})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87a962eca0a4485b0abb74967316207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd6dc53016d4a08ab1402d818265ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca525dbb288c41a7a29cc88561092d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"SBER_LORA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = utils.StoIHT(model.parameters(), k=400, \n",
    "                         approx=utils.approx_0, proj=utils.proj_0, lr=1e-1, prob=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import time\n",
    "Time = str(time.ctime()).replace(\"  \", \" \").replace(\" \", \"_\").replace(':', '-')\n",
    "\n",
    "ars = transformers.TrainingArguments(\n",
    "    # per_device_train_batch_size=1, \n",
    "    # gradient_accumulation_steps=16, \n",
    "    # warmup_steps=10, \n",
    "    max_steps=15, \n",
    "    # learning_rate=1e-3, \n",
    "    fp16=False, \n",
    "    output_dir=f\"my_lora/outputs/{Time}\", \n",
    "    use_cpu=True, \n",
    "    save_safetensors=False,\n",
    "    # report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    "    run_name=Time,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    args=ars,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15),\n",
    "    optimizers=[optimizer, None],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrey/Papers/Sber/wandb/run-20240808_192712-vkztod99</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/vkztod99' target=\"_blank\">Thu_Aug_8_19-27-09_2024</a></strong> to <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/vkztod99' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/vkztod99</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d124131106e642d18be070f2e7e820aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3376, 'grad_norm': 32.499393463134766, 'learning_rate': 0.09333333333333334, 'epoch': 0.03}\n",
      "{'loss': 5.0595, 'grad_norm': 54.098995208740234, 'learning_rate': 0.08666666666666667, 'epoch': 0.05}\n",
      "{'loss': 5.1413, 'grad_norm': 33.08547592163086, 'learning_rate': 0.08000000000000002, 'epoch': 0.08}\n",
      "{'loss': 5.9695, 'grad_norm': 43.597198486328125, 'learning_rate': 0.07333333333333333, 'epoch': 0.1}\n",
      "{'loss': 5.2205, 'grad_norm': 33.3675651550293, 'learning_rate': 0.06666666666666667, 'epoch': 0.13}\n",
      "{'loss': 3.8537, 'grad_norm': 29.69696044921875, 'learning_rate': 0.06, 'epoch': 0.15}\n",
      "{'loss': 5.4341, 'grad_norm': 46.81707000732422, 'learning_rate': 0.05333333333333334, 'epoch': 0.18}\n",
      "{'loss': 4.9852, 'grad_norm': 45.01075744628906, 'learning_rate': 0.04666666666666667, 'epoch': 0.21}\n",
      "{'loss': 5.1199, 'grad_norm': 36.69898986816406, 'learning_rate': 0.04000000000000001, 'epoch': 0.23}\n",
      "{'loss': 4.2691, 'grad_norm': 32.556095123291016, 'learning_rate': 0.03333333333333333, 'epoch': 0.26}\n",
      "{'loss': 5.0469, 'grad_norm': 40.635196685791016, 'learning_rate': 0.02666666666666667, 'epoch': 0.28}\n",
      "{'loss': 4.7089, 'grad_norm': 43.47609329223633, 'learning_rate': 0.020000000000000004, 'epoch': 0.31}\n",
      "{'loss': 4.7782, 'grad_norm': 36.269561767578125, 'learning_rate': 0.013333333333333334, 'epoch': 0.33}\n",
      "{'loss': 4.9959, 'grad_norm': 45.83007049560547, 'learning_rate': 0.006666666666666667, 'epoch': 0.36}\n",
      "{'loss': 5.9861, 'grad_norm': 52.36640548706055, 'learning_rate': 0.0, 'epoch': 0.38}\n",
      "{'train_runtime': 19.1066, 'train_samples_per_second': 6.281, 'train_steps_per_second': 0.785, 'train_loss': 4.993755435943603, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a459f9848e64bc19df1b44a88ccf3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▃▄▄▄▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▂█▂▅▂▁▆▅▃▂▄▅▃▆█</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▆▅▅▄▄▄▃▂▂▂▁</td></tr><tr><td>train/loss</td><td>▃▅▅█▅▁▆▅▅▂▅▄▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>3695403086352.0</td></tr><tr><td>train/epoch</td><td>0.38462</td></tr><tr><td>train/global_step</td><td>15</td></tr><tr><td>train/grad_norm</td><td>52.36641</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>5.9861</td></tr><tr><td>train_loss</td><td>4.99376</td></tr><tr><td>train_runtime</td><td>19.1066</td></tr><tr><td>train_samples_per_second</td><td>6.281</td></tr><tr><td>train_steps_per_second</td><td>0.785</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Thu_Aug_8_19-27-09_2024</strong> at: <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/vkztod99' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/vkztod99</a><br/> View project at: <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240808_192712-vkztod99/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.save_pretrained(output_dir, safe_serialization=False)\n",
    "ret = trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a25179e0c4453f8cd6d360eddc955e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 18.2847900390625,\n",
       " 'eval_runtime': 1.8194,\n",
       " 'eval_samples_per_second': 18.687,\n",
       " 'eval_steps_per_second': 2.748,\n",
       " 'epoch': 0.05128205128205128}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
