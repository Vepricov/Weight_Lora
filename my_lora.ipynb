{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available? False\n"
     ]
    }
   ],
   "source": [
    "# прикол для сервера\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = 'false'\n",
    "\n",
    "def set_seed(seed): # ставит сид\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def set_device(device_no: int): # выбирает GPU-шку и выводит название\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f\"cuda:{device_no}\")\n",
    "        print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "        print(\"We will use the GPU:\", torch.cuda.get_device_name(device_no))\n",
    "    else:\n",
    "        print(\"No GPU available, using the CPU instead.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "set_seed(18)\n",
    "# device = set_device(7)\n",
    "print(f'GPU available? {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForCausalLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n",
      "model device = cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "weight_type = torch.float32 # in gpu can be torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=weight_type,\n",
    "    load_in_8bit=False,\n",
    "    device_map='cpu',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(model)\n",
    "device = model.device\n",
    "print(f'model device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# class CastOutputToFloat(nn.Sequential):\n",
    "#     def forward(self, x): return super().forward(x).to(weight_type)\n",
    "# model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen layers: \n",
      "#1:  name = roberta.encoder.layer.0.attention.self.query\n",
      "#2:  name = roberta.encoder.layer.0.attention.self.value\n",
      "#3:  name = roberta.encoder.layer.1.attention.self.query\n",
      "#4:  name = roberta.encoder.layer.1.attention.self.value\n",
      "#5:  name = roberta.encoder.layer.2.attention.self.query\n",
      "#6:  name = roberta.encoder.layer.2.attention.self.value\n",
      "#7:  name = roberta.encoder.layer.3.attention.self.query\n",
      "#8:  name = roberta.encoder.layer.3.attention.self.value\n",
      "#9:  name = roberta.encoder.layer.4.attention.self.query\n",
      "#10:  name = roberta.encoder.layer.4.attention.self.value\n",
      "#11:  name = roberta.encoder.layer.5.attention.self.query\n",
      "#12:  name = roberta.encoder.layer.5.attention.self.value\n",
      "#13:  name = roberta.encoder.layer.6.attention.self.query\n",
      "#14:  name = roberta.encoder.layer.6.attention.self.value\n",
      "#15:  name = roberta.encoder.layer.7.attention.self.query\n",
      "#16:  name = roberta.encoder.layer.7.attention.self.value\n",
      "#17:  name = roberta.encoder.layer.8.attention.self.query\n",
      "#18:  name = roberta.encoder.layer.8.attention.self.value\n",
      "#19:  name = roberta.encoder.layer.9.attention.self.query\n",
      "#20:  name = roberta.encoder.layer.9.attention.self.value\n",
      "#21:  name = roberta.encoder.layer.10.attention.self.query\n",
      "#22:  name = roberta.encoder.layer.10.attention.self.value\n",
      "#23:  name = roberta.encoder.layer.11.attention.self.query\n",
      "#24:  name = roberta.encoder.layer.11.attention.self.value\n",
      "Overall chosen layers: 24\n"
     ]
    }
   ],
   "source": [
    "layer_filter = lambda name : ('attention' in name) and \\\n",
    "    (('query' in name) or ('value' in name)) and \\\n",
    "    ('bias' not in name)\n",
    "    \n",
    "print(f\"Chosen layers: \")\n",
    "chosen_layers = []\n",
    "for name, param in model.named_modules():\n",
    "    if layer_filter(name):\n",
    "        right_name = name\n",
    "        if model_name == \"FacebookAI/roberta-base\":\n",
    "            chosen_layers.append(name)\n",
    "        print(f\"#{len(chosen_layers)}:  name = {name}\")\n",
    "\n",
    "print(f\"Overall chosen layers: {len(chosen_layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_layers = chosen_layers[:2] # baby steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "for name, param in model.named_parameters():\n",
    "    # if name not in chosen_layers:\n",
    "    #     param.requires_grad = False\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32) \n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 124697433 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add adapter\n",
    "for i in range(0, len(chosen_layers) // 2):\n",
    "    model.roberta.encoder.layer[i].attention.self.query = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.query)\n",
    "    model.roberta.encoder.layer[i].attention.self.value = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1179648 || all params: 125877081 || trainable%: 0.9371427988547018\n"
     ]
    }
   ],
   "source": [
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrey/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andrey/miniconda3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0B637046-A38B-3A5C-80C6-E847C27DCCD5> /Users/andrey/miniconda3/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <EACD001F-FCB9-380E-AD73-D522177FC040> /Users/andrey/miniconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: I enjoy to <mask> in sberbank.\n",
      "#1: I enjoy to invest in sberbank. || score = 0.6629555225372314\n",
      "#2: I enjoy to work in sberbank. || score = 0.13997869193553925\n",
      "#3: I enjoy to trade in sberbank. || score = 0.02613656222820282\n",
      "#4: I enjoy to participate in sberbank. || score = 0.02500171773135662\n",
      "#5: I enjoy toiling in sberbank. || score = 0.020179422572255135\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentence = \"I enjoy to <mask> in sberbank.\"\n",
    "unmasker = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "predictions = unmasker(sentence)\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"#{i+1}: {predictions[i]['sequence']} || score = {predictions[i]['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['question', 'subject', 'choices', 'answer'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = 'cais/mmlu'\n",
    "dataset_config_name = 'philosophy'\n",
    "\n",
    "# Can be changed to wiki (as in Micrisoft):\n",
    "# dataset_name = 'wikitext'\n",
    "# dataset_config_name = 'wikitext-2-raw-v1'\n",
    "\n",
    "dataset = load_dataset(dataset_name, dataset_config_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____.\n",
      "subject: philosophy\n",
      "choices: ['the state', 'the justice system', 'the body', 'the soul']\n",
      "answer: 3\n"
     ]
    }
   ],
   "source": [
    "# only for MMLU\n",
    "num = 1\n",
    "print(f\"question: {dataset['test']['question'][num]}\")\n",
    "print(f\"subject: {dataset['test']['subject'][num]}\")\n",
    "print(f\"choices: {dataset['test']['choices'][num]}\")\n",
    "print(f\"answer: {dataset['test']['answer'][num]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "def make_mlm_dataset_form_mmlu(dataset, head=3):\n",
    "    dataset_list = []\n",
    "    for a in dataset:\n",
    "        q = a['question']\n",
    "        q = q.replace('_', '')\n",
    "        q += ' ' + a['choices'][a['answer']]\n",
    "        q = q.replace('.', '')\n",
    "        q = q.replace('  ', ' ')\n",
    "        q += '.'\n",
    "        dataset_list.append({\"text\" : q})\n",
    "\n",
    "    if head > 0:\n",
    "        print(\"Examples:\")\n",
    "    for i, a in enumerate(dataset_list[:head]):\n",
    "        print(f\"#{i+1}: {a['text']}\")\n",
    "\n",
    "    return_dataset = datasets.Dataset.from_list(dataset_list)\n",
    "    return return_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Examples:\n",
      "#1: Aesthetics deals with objects that are not essential to our existence.\n",
      "#2: For Socrates, an unexamined life is a tragedy because it results in grievous harm to the soul.\n",
      "#3: According to Kant, nothing can be called “good” without qualification except a good will.\n",
      "NUM ROWS = 311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST\n",
      "Examples:\n",
      "#1: One of the aims of philosophy is to think critically about whether there are good reasons for adopting our beliefs Reasons are considered \"good reasons\" if they are consistent with everyday experience and: take into account objections, are acceptable to impartial third parties, and avoid undesirable consequences.\n",
      "#2: The existence of a form of mental illness known as multiple personality disorder seems to suggest that the mind is divisible.\n",
      "#3: Singer’s argument begins with the assumption that: suffering and death from lack of food, shelter, and medical care are bad.\n",
      "NUM ROWS = 34\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VALIDATION\n",
      "Examples:\n",
      "#1: Psychological egoism is: a claim about human nature and the ways people are capable of behaving.\n",
      "#2: According to Moore’s “ideal utilitarianism,” the right action is the one that brings about the greatest amount of: good.\n",
      "#3: According to d'Holbach, people always act according to necessary natural laws.\n",
      "NUM ROWS = 5\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 34\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 311\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TRAIN')\n",
    "train = make_mlm_dataset_form_mmlu(dataset['test'])\n",
    "print(f'NUM ROWS = {len(train)}', '-'*100, sep='\\n')\n",
    "print('TEST')\n",
    "test = make_mlm_dataset_form_mmlu(dataset['validation'])\n",
    "print(f'NUM ROWS = {len(test)}', '-'*100, sep='\\n')\n",
    "print('VALIDATION')\n",
    "val = make_mlm_dataset_form_mmlu(dataset['dev'])\n",
    "print(f'NUM ROWS = {len(val)}', '-'*100, sep='\\n')\n",
    "dataset = datasets.DatasetDict({\"test\" : test,\n",
    "                                \"train\" : train,\n",
    "                                \"validation\" : val})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1924431f6ada4607bedc16aefd2e7ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72387ae0b9a14a3d9c87fee285564505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba925f08c6994a16a01d420201f51e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "### Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshkodnik\u001b[0m (\u001b[33mshkodnik-mipt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"SBER_LORA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1179648 || all params: 125877081 || trainable%: 0.9371427988547018\n"
     ]
    }
   ],
   "source": [
    "# reload model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=weight_type,\n",
    "    load_in_8bit=False,\n",
    "    device_map='cpu',\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    if param.ndim == 1:\n",
    "        param.data = param.data.to(torch.float32) \n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for i in range(0, len(chosen_layers) // 2):\n",
    "    model.roberta.encoder.layer[i].attention.self.query = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.query)\n",
    "    model.roberta.encoder.layer[i].attention.self.value = \\\n",
    "        utils.AdapterLayer(model.roberta.encoder.layer[i].attention.self.value)\n",
    "utils.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0.\n",
    "k = 400\n",
    "optimizer = utils.StoIHT(model.parameters(), k=k, \n",
    "                         approx=utils.approx_0, proj=utils.proj_0, lr=1e-1, prob=prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import time\n",
    "Time = str(time.ctime()).replace(\"  \", \" \").replace(\" \", \"_\").replace(':', '-')\n",
    "\n",
    "ars = transformers.TrainingArguments(\n",
    "    # per_device_train_batch_size=1, \n",
    "    # gradient_accumulation_steps=16, \n",
    "    # warmup_steps=10, \n",
    "    max_steps=15, \n",
    "    # learning_rate=1e-3, \n",
    "    fp16=False, \n",
    "    output_dir=f\"my_lora/outputs/{prob}_{k}\", \n",
    "    use_cpu=True, \n",
    "    save_safetensors=False,\n",
    "    # report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    "    run_name=f\"prob={prob}/k={k}\",\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    args=ars,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15),\n",
    "    optimizers=[optimizer, None],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/andrey/Papers/Sber/Sber_Lora/wandb/run-20240817_135105-8x2sjxfn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/8x2sjxfn' target=\"_blank\">prob=0.0/k=400</a></strong> to <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/8x2sjxfn' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/8x2sjxfn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: model-prob=0.0/k=400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f87b679f354f0d9e394d27f74e197a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prob=0.0/k=400</strong> at: <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/8x2sjxfn' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA/runs/8x2sjxfn</a><br/> View project at: <a href='https://wandb.ai/shkodnik-mipt/SBER_LORA' target=\"_blank\">https://wandb.ai/shkodnik-mipt/SBER_LORA</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240817_135105-8x2sjxfn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model.save_pretrained(output_dir, safe_serialization=False)\n",
    "try:\n",
    "    ret = trainer.train()\n",
    "    wandb.finish()\n",
    "except Exception as err:\n",
    "    print(\"ERROR!\")\n",
    "    print(err)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a25179e0c4453f8cd6d360eddc955e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 18.2847900390625,\n",
       " 'eval_runtime': 1.8194,\n",
       " 'eval_samples_per_second': 18.687,\n",
       " 'eval_steps_per_second': 2.748,\n",
       " 'epoch': 0.05128205128205128}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
