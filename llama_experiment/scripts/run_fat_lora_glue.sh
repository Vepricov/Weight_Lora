clear
CUDA_VISIBLE_DEVICES=2 python ./llama_experiment/run_llama.py \
                --dataset_name glue \
                --task_name sst2 \
                --model_name_or_path meta-llama/Llama-2-7b-hf \
                --per_device_train_batch_size 2 \
                --per_device_eval_batch_size 2 \
                --gradient_accumulation_steps 2 \
                --learning_rate tuned \
                --learning_rate_w 8e-0 \
                --lr_scheduler_type linear \
                --warmup_steps 50 \
                --max_steps 512 \
                --eval_steps 64 \
                --save_steps 256 \
                --ft_strategy LoRA \
                --lora_r 8 \
                --lora_dropout 0.05 \
                --lora_alpha 32 \
                --use_fat true \
                --do_evaluate true \
                --fat_step 5 \
                --max_fat_steps 2 \
                --lora_extention smart \
                --seed 18 \
                --report_to wandb
