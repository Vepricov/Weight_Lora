{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# from unsloth import FastLanguageModel\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "# from unsloth import is_bfloat16_supported\n",
    "# from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from dataclasses import dataclass\n",
    "from datasets import Dataset\n",
    "import torch_optimizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'active_adapters_1': 'value#5', 'active_adapters_10': 'value#11', 'active_adapters_2': 'value#6', 'active_adapters_3': 'value#7', 'active_adapters_4': 'value#8', 'active_adapters_5': 'value#9', 'active_adapters_6': 'key#10', 'active_adapters_7': 'value#10', 'active_adapters_8': 'query#11', 'active_adapters_9': 'key#11', 'epoch': 1.4593824228028502, 'eval_accuracy': 0.9472477064220184, 'eval_loss': 0.15586502850055695, 'eval_runtime': 2.349, 'eval_samples': 872, 'eval_samples_per_second': 371.224, 'eval_steps_per_second': 11.92, 'total_flos': 6499006338439680.0, 'train_loss': 0.2506979462923482, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.963231666666667, 'train_samples': 67349, 'train_samples_per_second': 149.445, 'train_steps_per_second': 0.778}\n",
      "{'active_adapters_1': 'value#8', 'active_adapters_2': 'value#9', 'active_adapters_3': 'value#10', 'active_adapters_4': 'key#11', 'active_adapters_5': 'value#11', 'epoch': 1.4593824228028502, 'eval_accuracy': 0.9426605504587156, 'eval_loss': 0.16940219700336456, 'eval_runtime': 2.4226, 'eval_samples': 872, 'eval_samples_per_second': 359.939, 'eval_steps_per_second': 11.558, 'total_flos': 6499006338439680.0, 'train_loss': 0.27623511309502646, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.966438333333334, 'train_samples': 67349, 'train_samples_per_second': 149.401, 'train_steps_per_second': 0.778}\n",
      "{'active_adapters_1': 'value#8', 'active_adapters_2': 'key#10', 'active_adapters_3': 'value#10', 'active_adapters_4': 'query#11', 'active_adapters_5': 'value#11', 'epoch': 0.2503259452411995, 'eval_accuracy': 0.8638120423108218, 'eval_loss': 0.3649160861968994, 'eval_runtime': 26.7252, 'eval_samples': 9832, 'eval_samples_per_second': 367.892, 'eval_steps_per_second': 11.525, 'total_flos': 6499791701803008.0, 'train_loss': 0.5594488628557883, 'train_memory_gb': 6.533195495605469, 'train_runtime': 14.232040000000001, 'train_samples': 392702, 'train_samples_per_second': 115.121, 'train_steps_per_second': 0.6}\n",
      "{'active_adapters_1': 'query#3', 'active_adapters_10': 'value#11', 'active_adapters_2': 'key#3', 'active_adapters_3': 'value#4', 'active_adapters_4': 'value#8', 'active_adapters_5': 'value#9', 'active_adapters_6': 'key#10', 'active_adapters_7': 'value#10', 'active_adapters_8': 'query#11', 'active_adapters_9': 'key#11', 'epoch': 0.2503259452411995, 'eval_accuracy': 0.870626525630594, 'eval_loss': 0.3444303274154663, 'eval_runtime': 26.7912, 'eval_samples': 9832, 'eval_samples_per_second': 366.987, 'eval_steps_per_second': 11.496, 'total_flos': 6499791701803008.0, 'train_loss': 0.5290017838706262, 'train_memory_gb': 6.533195495605469, 'train_runtime': 14.273683333333334, 'train_samples': 392702, 'train_samples_per_second': 114.785, 'train_steps_per_second': 0.598}\n",
      "{'active_adapters_1': 'value#10', 'epoch': 17.066666666666666, 'eval_combined_score': 0.8596423307311537, 'eval_loss': 0.6633880734443665, 'eval_pearson': 0.8454271660551687, 'eval_runtime': 4.0207, 'eval_samples': 1500, 'eval_samples_per_second': 373.071, 'eval_spearmanr': 0.8738574954071388, 'eval_steps_per_second': 11.69, 'total_flos': 6487311498406656.0, 'train_loss': 1.9646413220325485, 'train_memory_gb': 6.53318977355957, 'train_runtime': 34.68832833333333, 'train_samples': 5749, 'train_samples_per_second': 47.232, 'train_steps_per_second': 0.246}\n",
      "{'active_adapters_1': 'value#11', 'epoch': 26.713043478260868, 'eval_accuracy': 0.8308823529411765, 'eval_combined_score': 0.8566677522797876, 'eval_f1': 0.8824531516183987, 'eval_loss': 0.3816204369068146, 'eval_runtime': 1.1651, 'eval_samples': 408, 'eval_samples_per_second': 350.198, 'eval_steps_per_second': 11.158, 'total_flos': 6479104606801920.0, 'train_loss': 0.45998158876318485, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 32.98885, 'train_samples': 3668, 'train_samples_per_second': 49.665, 'train_steps_per_second': 0.259}\n",
      "{'active_adapters_1': 'value#10', 'epoch': 0.9383017715332926, 'eval_accuracy': 0.8724144243089877, 'eval_loss': 0.3167034983634949, 'eval_runtime': 14.981, 'eval_samples': 5463, 'eval_samples_per_second': 364.662, 'eval_steps_per_second': 11.414, 'total_flos': 6499733644247040.0, 'train_loss': 0.4622076007653959, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 12.730256666666666, 'train_samples': 104743, 'train_samples_per_second': 128.701, 'train_steps_per_second': 0.67}\n",
      "{'active_adapters_1': 'value#4', 'active_adapters_10': 'value#11', 'active_adapters_2': 'value#6', 'active_adapters_3': 'value#7', 'active_adapters_4': 'value#8', 'active_adapters_5': 'value#9', 'active_adapters_6': 'query#10', 'active_adapters_7': 'key#10', 'active_adapters_8': 'value#10', 'active_adapters_9': 'key#11', 'epoch': 11.462686567164178, 'eval_loss': 0.4278945326805115, 'eval_matthews_correlation': 0.670267087346549, 'eval_runtime': 2.9418, 'eval_samples': 1043, 'eval_samples_per_second': 354.55, 'eval_steps_per_second': 11.218, 'total_flos': 6481550999063040.0, 'train_loss': 0.2963522005302366, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 11.035423333333334, 'train_samples': 8551, 'train_samples_per_second': 148.467, 'train_steps_per_second': 0.773}\n",
      "{'active_adapters_1': 'value#8', 'active_adapters_2': 'value#9', 'active_adapters_3': 'key#10', 'active_adapters_4': 'value#10', 'active_adapters_5': 'value#11', 'epoch': 11.462686567164178, 'eval_loss': 0.3877233862876892, 'eval_matthews_correlation': 0.6490535130984493, 'eval_runtime': 2.8472, 'eval_samples': 1043, 'eval_samples_per_second': 366.321, 'eval_steps_per_second': 11.59, 'total_flos': 6481550999063040.0, 'train_loss': 0.353874961292604, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.97293, 'train_samples': 8551, 'train_samples_per_second': 149.313, 'train_steps_per_second': 0.778}\n",
      "{'active_adapters_1': 'value#10', 'epoch': 0.2503259452411995, 'eval_accuracy': 0.7911920260374288, 'eval_loss': 0.5689704418182373, 'eval_runtime': 26.8703, 'eval_samples': 9832, 'eval_samples_per_second': 365.906, 'eval_steps_per_second': 11.462, 'total_flos': 6499791701803008.0, 'train_loss': 0.818801112473011, 'train_memory_gb': 6.533195495605469, 'train_runtime': 14.210668333333333, 'train_samples': 392702, 'train_samples_per_second': 115.294, 'train_steps_per_second': 0.6}\n",
      "{'active_adapters_1': 'value#11', 'epoch': 1.4593824228028502, 'eval_accuracy': 0.8876146788990825, 'eval_loss': 0.27126485109329224, 'eval_runtime': 2.3448, 'eval_samples': 872, 'eval_samples_per_second': 371.887, 'eval_steps_per_second': 11.941, 'total_flos': 6499006338439680.0, 'train_loss': 0.3876057167944964, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 11.017525000000001, 'train_samples': 67349, 'train_samples_per_second': 148.709, 'train_steps_per_second': 0.775}\n",
      "{'active_adapters_1': 'value#6', 'active_adapters_10': 'value#11', 'active_adapters_2': 'value#7', 'active_adapters_3': 'value#8', 'active_adapters_4': 'value#9', 'active_adapters_5': 'query#10', 'active_adapters_6': 'key#10', 'active_adapters_7': 'value#10', 'active_adapters_8': 'query#11', 'active_adapters_9': 'key#11', 'epoch': 0.27016093571365757, 'eval_accuracy': 0.8727924808310661, 'eval_combined_score': 0.854472636605281, 'eval_f1': 0.836152792379496, 'eval_loss': 0.2930339574813843, 'eval_runtime': 108.0279, 'eval_samples': 40430, 'eval_samples_per_second': 374.255, 'eval_steps_per_second': 11.701, 'total_flos': 6499733644247040.0, 'train_loss': 0.3584921188012231, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 25.027408333333334, 'train_samples': 363846, 'train_samples_per_second': 65.464, 'train_steps_per_second': 0.341}\n",
      "{'active_adapters_1': 'value#4', 'active_adapters_10': 'value#11', 'active_adapters_2': 'value#6', 'active_adapters_3': 'value#7', 'active_adapters_4': 'value#8', 'active_adapters_5': 'value#9', 'active_adapters_6': 'query#10', 'active_adapters_7': 'key#10', 'active_adapters_8': 'value#10', 'active_adapters_9': 'key#11', 'epoch': 17.066666666666666, 'eval_combined_score': 0.8951776863642001, 'eval_loss': 0.5758686065673828, 'eval_pearson': 0.8896787458237689, 'eval_runtime': 4.0012, 'eval_samples': 1500, 'eval_samples_per_second': 374.885, 'eval_spearmanr': 0.9006766269046314, 'eval_steps_per_second': 11.746, 'total_flos': 6487311498406656.0, 'train_loss': 1.1014063967741095, 'train_memory_gb': 6.53318977355957, 'train_runtime': 11.096708333333334, 'train_samples': 5749, 'train_samples_per_second': 147.647, 'train_steps_per_second': 0.769}\n",
      "{'active_adapters_1': 'value#8', 'active_adapters_2': 'value#9', 'active_adapters_3': 'key#10', 'active_adapters_4': 'value#10', 'active_adapters_5': 'value#11', 'epoch': 17.066666666666666, 'eval_combined_score': 0.8920177149011603, 'eval_loss': 0.5807738304138184, 'eval_pearson': 0.8859022378356229, 'eval_runtime': 4.0535, 'eval_samples': 1500, 'eval_samples_per_second': 370.049, 'eval_spearmanr': 0.8981331919666977, 'eval_steps_per_second': 11.595, 'total_flos': 6487311498406656.0, 'train_loss': 1.1493584814597853, 'train_memory_gb': 6.53318977355957, 'train_runtime': 11.102046666666666, 'train_samples': 5749, 'train_samples_per_second': 147.576, 'train_steps_per_second': 0.769}\n",
      "{'active_adapters_1': 'value#8', 'active_adapters_2': 'value#9', 'active_adapters_3': 'key#10', 'active_adapters_4': 'value#10', 'active_adapters_5': 'value#11', 'epoch': 0.27016093571365757, 'eval_accuracy': 0.8685134800890428, 'eval_combined_score': 0.8488454289905368, 'eval_f1': 0.8291773778920308, 'eval_loss': 0.30148443579673767, 'eval_runtime': 188.0324, 'eval_samples': 40430, 'eval_samples_per_second': 215.016, 'eval_steps_per_second': 6.722, 'total_flos': 6499733644247040.0, 'train_loss': 0.3741751284105703, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 42.042535, 'train_samples': 363846, 'train_samples_per_second': 38.97, 'train_steps_per_second': 0.203}\n",
      "{'active_adapters_1': 'value#8', 'active_adapters_2': 'value#9', 'active_adapters_3': 'value#10', 'active_adapters_4': 'key#11', 'active_adapters_5': 'value#11', 'epoch': 26.713043478260868, 'eval_accuracy': 0.8799019607843137, 'eval_combined_score': 0.897342284739983, 'eval_f1': 0.9147826086956522, 'eval_loss': 0.35847482085227966, 'eval_runtime': 1.1259, 'eval_samples': 408, 'eval_samples_per_second': 362.372, 'eval_steps_per_second': 11.546, 'total_flos': 6479104606801920.0, 'train_loss': 0.226518808703986, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.825195, 'train_samples': 3668, 'train_samples_per_second': 151.351, 'train_steps_per_second': 0.788}\n",
      "{'active_adapters_1': 'value#4', 'active_adapters_10': 'value#11', 'active_adapters_2': 'value#7', 'active_adapters_3': 'value#8', 'active_adapters_4': 'value#9', 'active_adapters_5': 'query#10', 'active_adapters_6': 'key#10', 'active_adapters_7': 'value#10', 'active_adapters_8': 'query#11', 'active_adapters_9': 'key#11', 'epoch': 26.713043478260868, 'eval_accuracy': 0.8774509803921569, 'eval_combined_score': 0.8937614614191001, 'eval_f1': 0.9100719424460432, 'eval_loss': 0.3722935616970062, 'eval_runtime': 1.1169, 'eval_samples': 408, 'eval_samples_per_second': 365.281, 'eval_steps_per_second': 11.639, 'total_flos': 6479104606801920.0, 'train_loss': 0.17981932511975174, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.870336666666667, 'train_samples': 3668, 'train_samples_per_second': 150.722, 'train_steps_per_second': 0.785}\n",
      "{'active_adapters_1': 'value#11', 'epoch': 0.27016093571365757, 'eval_accuracy': 0.8306455602275538, 'eval_combined_score': 0.8059911925942163, 'eval_f1': 0.7813368249608789, 'eval_loss': 0.37401559948921204, 'eval_runtime': 207.1166, 'eval_samples': 40430, 'eval_samples_per_second': 195.204, 'eval_steps_per_second': 6.103, 'total_flos': 6499733644247040.0, 'train_loss': 0.4499928216682747, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 33.36299666666667, 'train_samples': 363846, 'train_samples_per_second': 49.108, 'train_steps_per_second': 0.256}\n",
      "{'active_adapters_1': 'query#0', 'active_adapters_2': 'key#3', 'active_adapters_3': 'value#3', 'active_adapters_4': 'query#4', 'active_adapters_5': 'value#5', 'epoch': 39.38461538461539, 'eval_accuracy': 0.555956678700361, 'eval_loss': 0.7349116802215576, 'eval_runtime': 0.7779, 'eval_samples': 277, 'eval_samples_per_second': 356.101, 'eval_steps_per_second': 11.57, 'total_flos': 6484261866163200.0, 'train_loss': 0.5540909016272053, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.817396666666667, 'train_samples': 2490, 'train_samples_per_second': 151.46, 'train_steps_per_second': 0.789}\n",
      "{'active_adapters_1': 'value#6', 'active_adapters_2': 'value#7', 'active_adapters_3': 'value#8', 'active_adapters_4': 'value#10', 'active_adapters_5': 'value#11', 'epoch': 0.9383017715332926, 'eval_accuracy': 0.9136005857587406, 'eval_loss': 0.22189098596572876, 'eval_runtime': 15.0713, 'eval_samples': 5463, 'eval_samples_per_second': 362.478, 'eval_steps_per_second': 11.346, 'total_flos': 6499733644247040.0, 'train_loss': 0.37031027625198476, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 12.742536666666666, 'train_samples': 104743, 'train_samples_per_second': 128.577, 'train_steps_per_second': 0.67}\n",
      "{'active_adapters_1': 'value#2', 'active_adapters_10': 'value#11', 'active_adapters_2': 'query#3', 'active_adapters_3': 'value#3', 'active_adapters_4': 'value#6', 'active_adapters_5': 'value#7', 'active_adapters_6': 'value#8', 'active_adapters_7': 'value#9', 'active_adapters_8': 'value#10', 'active_adapters_9': 'key#11', 'epoch': 0.9383017715332926, 'eval_accuracy': 0.9167124290682775, 'eval_loss': 0.21271362900733948, 'eval_runtime': 14.9044, 'eval_samples': 5463, 'eval_samples_per_second': 366.537, 'eval_steps_per_second': 11.473, 'total_flos': 6499733644247040.0, 'train_loss': 0.3519202568277251, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 12.659564999999999, 'train_samples': 104743, 'train_samples_per_second': 129.42, 'train_steps_per_second': 0.674}\n",
      "{'active_adapters_1': 'query#4', 'epoch': 39.38461538461539, 'eval_accuracy': 0.5270758122743683, 'eval_loss': 0.6926156878471375, 'eval_runtime': 0.7854, 'eval_samples': 277, 'eval_samples_per_second': 352.674, 'eval_steps_per_second': 11.459, 'total_flos': 6484261866163200.0, 'train_loss': 0.6936817718669772, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.820921666666667, 'train_samples': 2490, 'train_samples_per_second': 151.41, 'train_steps_per_second': 0.789}\n",
      "{'active_adapters_1': 'value#11', 'epoch': 11.462686567164178, 'eval_loss': 0.43724122643470764, 'eval_matthews_correlation': 0.5624034645452709, 'eval_runtime': 2.8239, 'eval_samples': 1043, 'eval_samples_per_second': 369.354, 'eval_steps_per_second': 11.686, 'total_flos': 6481550999063040.0, 'train_loss': 0.4418853195384145, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.976471666666667, 'train_samples': 8551, 'train_samples_per_second': 149.265, 'train_steps_per_second': 0.777}\n",
      "{'active_adapters_1': 'query#0', 'active_adapters_10': 'value#5', 'active_adapters_2': 'query#1', 'active_adapters_3': 'key#3', 'active_adapters_4': 'value#3', 'active_adapters_5': 'query#4', 'active_adapters_6': 'key#4', 'active_adapters_7': 'value#4', 'active_adapters_8': 'query#5', 'active_adapters_9': 'key#5', 'epoch': 39.38461538461539, 'eval_accuracy': 0.5342960288808665, 'eval_loss': 0.8279609680175781, 'eval_runtime': 0.7816, 'eval_samples': 277, 'eval_samples_per_second': 354.389, 'eval_steps_per_second': 11.514, 'total_flos': 6484261866163200.0, 'train_loss': 0.4925111113698222, 'train_memory_gb': 6.5331926345825195, 'train_runtime': 10.85097, 'train_samples': 2490, 'train_samples_per_second': 150.991, 'train_steps_per_second': 0.786}\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "INTERSTING_K = [1, 5, 10]\n",
    "NUM_INTERESTING_FILES = 0\n",
    "for folder in os.listdir(\"./train_outputs\"):\n",
    "    flag = True\n",
    "    for k in INTERSTING_K:\n",
    "        if f\"[WeightLoRA, k={k}]\" in folder:\n",
    "            flag = False\n",
    "    if flag:\n",
    "        continue\n",
    "    NUM_INTERESTING_FILES += 1\n",
    "    with open(f\"./train_outputs/{folder}/all_results.json\", \"r\") as f:\n",
    "        all_results = json.load(f)\n",
    "        #print(all_results)\n",
    "        for key in all_results.keys():\n",
    "            if \"active_adapters_\" in key:\n",
    "                tmp = all_results[key].split(\".\")\n",
    "                all_results[key] = f\"{tmp[8].split('_')[0]}#{tmp[5]}\"\n",
    "        print(all_results)\n",
    "    with open(f\"./train_outputs/{folder}/all_results.json\", \"w\") as f:\n",
    "        json.dump(all_results, f, sort_keys=True, indent=4)\n",
    "print(NUM_INTERESTING_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cola</th>\n",
       "      <th>mnli</th>\n",
       "      <th>mrpc</th>\n",
       "      <th>qnli</th>\n",
       "      <th>qqp</th>\n",
       "      <th>rte</th>\n",
       "      <th>sst2</th>\n",
       "      <th>stsb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cola</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mnli</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrpc</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qnli</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qqp</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rte</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sst2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stsb</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cola  mnli  mrpc  qnli  qqp  rte  sst2  stsb\n",
       "cola   NaN   1.0   NaN   NaN  NaN  NaN   NaN   NaN\n",
       "mnli   NaN   NaN   NaN   NaN  NaN  NaN   NaN   NaN\n",
       "mrpc   NaN   NaN   NaN   NaN  NaN  NaN   NaN   NaN\n",
       "qnli   NaN   NaN   NaN   NaN  NaN  NaN   NaN   NaN\n",
       "qqp    NaN   NaN   NaN   NaN  NaN  NaN   NaN   NaN\n",
       "rte    NaN   NaN   NaN   NaN  NaN  NaN   NaN   NaN\n",
       "sst2   NaN   NaN   NaN   NaN  NaN  NaN   NaN   NaN\n",
       "stsb   NaN   NaN   NaN   NaN  NaN  NaN   NaN   NaN"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=TASK_NAMES, index=TASK_NAMES, dtype=float)\n",
    "df.loc[\"cola\", \"mnli\"] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "TASK_NAMES = [\"cola\", \"mnli\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\"]\n",
    "INTERSTING_K = [1, 5, 10]\n",
    "df_all = {}\n",
    "for k in INTERSTING_K:\n",
    "    df_all[k] = pd.DataFrame(columns=TASK_NAMES, index=TASK_NAMES, dtype=float)\n",
    "    for task_name_1 in TASK_NAMES:\n",
    "        folder_1 = f\"./train_outputs/[WeightLoRA, k={k}] {task_name_1}\"\n",
    "        with open(f\"{folder_1}/all_results.json\", \"r\") as f:\n",
    "            all_results = json.load(f)\n",
    "            active_adapters_1 = []\n",
    "            for key in all_results.keys():\n",
    "                if \"active_adapters_\" in key:\n",
    "                    active_adapters_1.append(all_results[key])\n",
    "        for task_name_2 in TASK_NAMES:\n",
    "            folder_2 = f\"./train_outputs/[WeightLoRA, k={k}] {task_name_2}\"\n",
    "            with open(f\"{folder_2}/all_results.json\", \"r\") as f:\n",
    "                all_results = json.load(f)\n",
    "                active_adapters_2 = []\n",
    "                for key in all_results.keys():\n",
    "                    if \"active_adapters_\" in key:\n",
    "                        active_adapters_2.append(all_results[key])\n",
    "            v = sum(x == y for x, y in zip(active_adapters_1, active_adapters_2))\n",
    "            df_all[k].loc[task_name_1, task_name_2] = float(v) / k\n",
    "            df_all[k].loc[task_name_2, task_name_1] = float(v) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0IAAALmCAYAAABvpCQ4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKvUlEQVR4nO3deXiU9bU48DNhCYuAKBLQq0aLBS2KKBVxedy4YBcs4kLVFkvVViyojStVCLiUqoVaWxWlxeX+2oriUuuCKBesVtQKYvUW0VaUVgJELYKoQTPz+8Pb3KZEyQwTJpn38/F5n4d8511OXonmzDlz3lQmk8kEAABAgpQUOgAAAICtTSIEAAAkjkQIAABIHIkQAACQOBIhAAAgcSRCAABA4kiEAACAxJEIAQAAiSMRAgAAEkciBAAAJI5ECAAAKJjf//73MWzYsNhxxx0jlUrFfffdt9ljFixYEPvtt1+UlpZGr1694tZbb836uhIhAACgYDZs2BD9+vWL66+/vlH7L1++PL7yla/EEUccEUuWLIlzzz03Tj/99HjkkUeyum4qk8lkcgkYAAAgn1KpVNx7770xfPjwT93noosuigcffDBeeumlurWvf/3rsXbt2pgzZ06jr6UiBAAA5FVNTU2sW7eu3lZTU5OXcy9cuDAGDx5cb23o0KGxcOHCrM7TOi/R5MFHb71W6BCKzm/6TSx0CEXnpTa1hQ4BGmVpen2hQyg6D696vtAhFJ03D9qj0CEUne1mzyx0CEWpTbfdCx1CozSn36en/Pz2mDx5cr21ysrKmDRp0hafe9WqVVFWVlZvraysLNatWxcffPBBtG/fvlHnaTaJEAAAUBzGjx8fFRUV9dZKS0sLFE3DJEIAAEBelZaWNlni06NHj1i9enW9tdWrV0fnzp0bXQ2KkAgBAEBxSCejhX/QoEHx0EMP1Vt79NFHY9CgQVmdx7AEAACgYN57771YsmRJLFmyJCI+GY+9ZMmSWLFiRUR80mY3atSouv3PPPPMeO211+LCCy+Ml19+OW644Ya488474/vf/35W15UIAQAABfPcc89F//79o3///hERUVFREf3794+JEz8Z/FVVVVWXFEVE7LbbbvHggw/Go48+Gv369YupU6fGL37xixg6dGhW19UaBwAAxSCTLnQEOTn88MPjsx5teuuttzZ4zPPPb9k0TxUhAAAgcVSEAACgGKRbZkWoUFSEAACAxJEIAQAAiaM1DgAAikCmhQ5LKBQVIQAAIHEkQgAAQOJojQMAgGJgalxWVIQAAIDEkQgBAACJozUOAACKgalxWVERAgAAEkdFCAAAikG6ttARtCgqQgAAQOJIhAAAgMTRGgcAAMXAsISsqAgBAACJIxECAAASR2scAAAUg7TWuGyoCAEAAImjIgQAAEUgY1hCVlSEAACAxJEIAQAAiaM1DgAAioFhCVlREQIAABJniypCf/7zn2PFihWxcePGeuvHHHPMFgUFAADQlHJKhF577bU49thj48UXX4xUKhWZTCYiIlKpVERE1NbW5i9CAABg80yNy0pOrXHnnHNO7LbbbrFmzZro0KFD/M///E/8/ve/jwEDBsSCBQvyHCIAAEB+5VQRWrhwYfz3f/93dOvWLUpKSqKkpCQOOeSQmDJlSpx99tnx/PPP5ztOAADgs6R1ZWUjp4pQbW1tdOrUKSIiunXrFitXroyIiF133TWWLVuWv+gAAACaQE4Vob59+8YLL7wQu+22WwwcODCuvvrqaNu2bdx8882x++675ztGAACAvMopEbr00ktjw4YNERFx2WWXxVe/+tU49NBDY/vtt49Zs2blNUAAAKARDEvISk6J0NChQ+v+3KtXr3j55ZfjnXfeia5du9ZNjgMAAGiutug5Qv9qu+22y9epAAAAmlSjE6ERI0Y0+qT33HNPTsEAAAA5SmuNy0ajE6EuXbo0ZRwAAABbTaMToVtuuSVvF62pqYmampp6ayU1NVFaWpq3awAAAHyanJ4j9E/V1dXx5JNPxpNPPhnV1dWNPm7KlCnRpUuXettVP52+JaEAAECyZdLNZ2sBchqWsGHDhhg3blzcfvvtkf7fXsRWrVrFqFGj4mc/+1l06NDhM48fP358VFRU1FsrWf9mLqEAAABkLaeKUEVFRTz++OPxu9/9LtauXRtr166N3/72t/H444/Heeedt9njS0tLo3PnzvU2bXEAALAF0unms7UAOVWE7r777pg9e3YcfvjhdWtf/vKXo3379nHiiSfGjTfemK/4AAAA8i6nitD7778fZWVlm6x379493n///S0OCgAAoCnllAgNGjQoKisr48MPP6xb++CDD2Ly5MkxaNCgvAUHAAA0TiZT22y2liCn1rhrr702jj766PiP//iP6NevX0REvPDCC1FaWhpz587Na4AAAAD5llMitPfee8err74av/rVr+Lll1+OiIiTTjopTjnllGjfvn1eAwQAAMi3nBKhKVOmRFlZWZxxxhn11mfOnBnV1dVx0UUX5SU4AACgkVrI83uai5w+I3TTTTdFnz59Nln/whe+ENOnezAqAADQvOVUEVq1alX07Nlzk/UddtghqqqqtjgoAAAgSy3k+T3NRU4VoZ133jn+8Ic/bLL+hz/8IXbcccctDgoAAKAp5VQROuOMM+Lcc8+Njz76KI488siIiJg3b15ceOGFcd555+U1QAAAgHzLKRG64IIL4u23346zzjorNm7cGBER7dq1i4suuijGjx+f1wABAIBGMCwhKzklQqlUKq666qqYMGFCLF26NNq3bx977LFHlJaW5js+AACAvMspEfqnbbbZJr74xS/mKxYAAICtYosSIQAAoJlI1xY6ghYlp6lxAAAALZlECAAASBytcQAAUAxMjcuKihAAAJA4KkIAAFAM0ipC2VARAgAAEkciBAAAJI7WOAAAKAaGJWRFRQgAAEgciRAAAJA4WuMAAKAYmBqXFRUhAAAgcVSEAACgGKgIZUVFCAAASByJEAAAkDha4wAAoAhkMrWFDqFFURECAAASRyIEAAAkjtY4AAAoBqbGZUVFCAAASBwVIQAAKAYZFaFsqAgBAACJIxECAAASR2scAAAUA8MSsqIiBAAAJI5ECAAASJxm0xr3m34TCx1C0TnphcsKHULReWHfikKHUHQml3xc6BCKUmW62fznvWg8XOgAilDH/ToXOoSi4/eppjHqzf9X6BAax9S4rKgIAQAAiSMRAgAAEkfvBAAAFANT47KiIgQAACSOihAAABQDwxKyoiIEAAAkjkQIAABIHK1xAABQDAxLyIqKEAAAkDgSIQAAIHG0xgEAQDHQGpcVFSEAACBxVIQAAKAYeI5QVlSEAACAxJEIAQAAiaM1DgAAioFhCVlREQIAABJHIgQAACSO1jgAACgGpsZlRUUIAABIHIkQAACQOFrjAACgGJgalxUVIQAAIHFUhAAAoBgYlpAVFSEAACBxJEIAAEDiaI0DAIBiYFhCVlSEAACAxJEIAQAAiaM1DgAAioHWuKyoCAEAAInT6IrQiBEj4tZbb43OnTvHiBEjPnPfe+65Z4sDAwAAspDJFDqCFqXRiVCXLl0ilUrV/RkAAKClanQidMsttzT4ZwAAgJbGsAQAACgGhiVkpdGJUP/+/eta4zZn8eLFOQcEAADQ1BqdCA0fPrwJwwAAANh6Gp0IVVZWNmUcAADAltAal5Ut+ozQxo0bY82aNZH+t5u+yy67bFFQAAAATSmnROiVV16J0047LZ566ql665lMJlKpVNTW1uYlOAAAgKaQUyI0evToaN26dTzwwAPRs2fPRg9RAAAAmkhGa1w2ckqElixZEosWLYo+ffrkdNGampqoqampt/ZRpjbapFrldD4AAIBslORy0F577RVvvfVWzhedMmVKdOnSpd72wPr/yfl8AACQeOl089lagJwSoauuuiouvPDCWLBgQbz99tuxbt26etvmjB8/Pt59991621c7fSGXUAAAALKWU2vc4MGDIyLiyCOPrPf5oMYOSygtLY3S0tJ6a9riAACArSWnRGj+/Pn5jgMAANgSmUyhI2hRckqEDjvssPjwww/jT3/6U4PPEQIAAGjOcvqM0Jw5c2KXXXaJAw88MI455pgYPnx43XbsscfmO0YAAKDIXX/99VFeXh7t2rWLgQMHxrPPPvuZ+1977bXRu3fvaN++fey8887x/e9/Pz788MNGXy+nRGjcuHFxwgknRFVVVaTT6Xqbh6kCAEABFHpS3BZMjZs1a1ZUVFREZWVlLF68OPr16xdDhw6NNWvWNLj/r3/967j44oujsrIyli5dGr/85S9j1qxZ8YMf/KDR18wpEVq9enVUVFREWVlZLocDAADUmTZtWpxxxhkxevTo2GuvvWL69OnRoUOHmDlzZoP7P/XUU3HwwQfHySefHOXl5TFkyJA46aSTNltF+lc5JULHH398LFiwIJdDAQCAplDoKtC/bDU1NZs8YqempqbBsDdu3BiLFi2qm0wdEVFSUhKDBw+OhQsXNnjMQQcdFIsWLapLfF577bV46KGH4stf/nKjb1dOwxJ+/vOfxwknnBBPPPFE7L333tGmTZt6r5999tm5nBYAACgCU6ZMicmTJ9dbq6ysjEmTJm2y71tvvRW1tbWbdJuVlZXFyy+/3OD5Tz755HjrrbfikEMOiUwmEx9//HGceeaZWbXG5ZQI/eY3v4m5c+dGu3btYsGCBfWeJZRKpSRCAACQYOPHj4+Kiop6a//+HNEtsWDBgvjhD38YN9xwQwwcODD+8pe/xDnnnBOXX355TJgwoVHnyCkRuuSSS2Ly5Mlx8cUXR0lJTt11AABAPmWazyNtSktLG534dOvWLVq1ahWrV6+ut7569ero0aNHg8dMmDAhvvnNb8bpp58eERF77713bNiwIb7zne/EJZdc0qgcJacsZuPGjTFy5EhJEAAAsEXatm0b+++/f8ybN69uLZ1Ox7x582LQoEENHvP+++9vkou0atUqIiIyjXywbE6ZzKmnnhqzZs3K5VAAAIB6KioqYsaMGXHbbbfF0qVLY8yYMbFhw4YYPXp0RESMGjUqxo8fX7f/sGHD4sYbb4w77rgjli9fHo8++mhMmDAhhg0bVpcQbU5OrXG1tbVx9dVXxyOPPBL77LPPJsMSpk2blstpAQCAHGXSjauENEcjR46M6urqmDhxYqxatSr23XffmDNnTt0AhRUrVtSrAF166aWRSqXi0ksvjTfffDN22GGHGDZsWFx55ZWNvmZOidCLL74Y/fv3j4iIl156qd5r/zo4AQAAoDHGjh0bY8eObfC1f390T+vWraOysjIqKytzvl5OidD8+fNzviAAANAE0s1nWEJLYNoBAACQOBIhAAAgcXJqjQMAAJqZZvQcoZZARQgAAEgciRAAAJA4WuMAAKAYtODnCBWCihAAAJA4EiEAACBxtMYBAEAx8EDVrKgIAQAAiaMiBAAAxUBFKCsqQgAAQOJIhAAAgMTRGgcAAMUg4zlC2VARAgAAEkciBAAAJI7WOAAAKAamxmVFRQgAAEgcFSEAACgGacMSsqEiBAAAJI5ECAAASBytcQAAUAwyhiVkQ0UIAABIHIkQAACQOFrjAACgGJgalxUVIQAAIHEkQgAAQOI0m9a4l9rUFjqEovPCvhWFDqHo9FsyrdAhFJ/9zi50BEWp710jCx1C8Tnsj4WOoOi8fKf3Y/PN71PJlkmbGpcN/wUCAAASp9lUhAAAgC1gWEJWVIQAAIDEkQgBAACJozUOAACKQcawhGyoCAEAAIkjEQIAABJHaxwAABQDU+OyoiIEAAAkjooQAAAUg7RhCdlQEQIAABJHIgQAACSO1jgAACgGhiVkRUUIAABIHIkQAACQOFrjAACgGGRMjcuGihAAAJA4KkIAAFAMDEvIiooQAACQOBIhAAAgcbTGAQBAEcikDUvIhooQAACQOBIhAAAgcbTGAQBAMTA1LisqQgAAQOJIhAAAgMTRGgcAAMVAa1xWVIQAAIDEySkROvvss+O6667bZP3nP/95nHvuuVsaEwAAkK1MuvlsLUBOidDdd98dBx988CbrBx10UMyePXuLgwIAAGhKOSVCb7/9dnTp0mWT9c6dO8dbb721xUEBAAA0pZwSoV69esWcOXM2WX/44Ydj99133+KgAACALKUzzWdrAXKaGldRURFjx46N6urqOPLIIyMiYt68eTF16tS49tpr8xkfAABA3uWUCH3729+OmpqauPLKK+Pyyy+PiIjy8vK48cYbY9SoUXkNEAAAIN9yfo7QmDFjYsyYMVFdXR3t27ePbbbZJp9xAQAAWci0kJa05mKLHqi6Zs2aWLZsWURE9OnTJ3bYYYe8BAUAANCUckqE1q9fH2eddVb85je/iXT6kznhrVq1ipEjR8b111/f4EQ5AACgCakIZSWnqXGnn356PPPMM/Hggw/G2rVrY+3atfHAAw/Ec889F9/97nfzHSMAAEBe5VQReuCBB+KRRx6JQw45pG5t6NChMWPGjDj66KPzFhwAAEBTyCkR2n777Rtsf+vSpUt07dp1i4MCAACy9L8fWaFxcmqNu/TSS6OioiJWrVpVt7Zq1aq44IILYsKECXkLDgAAoCnkVBG68cYb4y9/+Uvssssuscsuu0RExIoVK6K0tDSqq6vjpptuqtt38eLFmxxfU1MTNTU19dY+ztRG61SrXMIBAADISk6J0PDhw7foolOmTInJkyfXWzuoyxfi4G333qLzAgBAYpkal5WsE6Ha2to44ogjYp999oltt902p4uOHz8+Kioq6q1N2vv0nM4FAACQrawToVatWsWQIUNi6dKlOSdCpaWlUVpaWj8QbXEAAMBWklNrXN++feO1116L3XbbLd/xAAAAudAal5WcpsZdccUVcf7558cDDzwQVVVVsW7dunobAABAc5ZTRejLX/5yREQcc8wxkUql6tYzmUykUqmora3NT3QAAECjZDIqQtnIKRGaP39+vuMAAADYanJKhA477LD48MMP409/+lOsWbMm0p5iCwAAtCA5JUJz5syJUaNGxVtvvbXJa1rjAACgAAxLyEpOwxLGjRsXJ5xwQlRVVUU6na63SYIAAIDmLqdEaPXq1VFRURFlZWX5jgcAAKDJ5ZQIHX/88bFgwYI8hwIAAOQsnWk+WwuQ02eEfv7zn8cJJ5wQTzzxROy9997Rpk2beq+fffbZeQkOAACgKeSUCP3mN7+JuXPnRrt27WLBggX1niWUSqUkQgAAsJVlWkglprnIKRG65JJLYvLkyXHxxRdHSUlO3XUAAAAFk1MWs3Hjxhg5cqQkCAAAaJFyymROPfXUmDVrVr5jAQAAclXoAQlJGJZQW1sbV199dTzyyCOxzz77bDIsYdq0aXkJDgAAoCnklAi9+OKL0b9//4iIeOmll+q99q+DEwAAAJqjnBKh+fPn5zsOAABgS6QLHUDLYtoBAACQODlVhAAAgObFc4SyoyIEAAAkjkQIAABIHK1xAABQDLTGZUVFCAAASByJEAAAkDha4wAAoBh4jlBWVIQAAIDEkQgBAACJozUOAACKgAeqZkdFCAAASBwVIQAAKAaGJWRFRQgAAEgciRAAAJA4WuMAAKAIGJaQHRUhAAAgcSRCAABA4miNAwCAYmBqXFZUhAAAgMRREQIAgCKQURHKiooQAACQOBIhAAAgcbTGFbHJJR8XOoTis9/ZhY6g6Nyz+LpCh1CURvi7Sgvg/1P5t2ehA6CwtMZlRUUIAABIHIkQAACQOFrjAACgCJgalx0VIQAAIHEkQgAAQOJojQMAgGKgNS4rKkIAAEDiqAgBAEARMCwhOypCAABA4kiEAACAxNEaBwAARUBrXHZUhAAAgMSRCAEAAAV3/fXXR3l5ebRr1y4GDhwYzz777Gfuv3bt2vje974XPXv2jNLS0vj85z8fDz30UKOvpzUOAACKQEtujZs1a1ZUVFTE9OnTY+DAgXHttdfG0KFDY9myZdG9e/dN9t+4cWP853/+Z3Tv3j1mz54dO+20U7zxxhux7bbbNvqaEiEAAKCgpk2bFmeccUaMHj06IiKmT58eDz74YMycOTMuvvjiTfafOXNmvPPOO/HUU09FmzZtIiKivLw8q2tqjQMAgGKQSTWbraamJtatW1dvq6mpaTDsjRs3xqJFi2Lw4MF1ayUlJTF48OBYuHBhg8fcf//9MWjQoPje974XZWVl0bdv3/jhD38YtbW1jb5dEiEAACCvpkyZEl26dKm3TZkypcF933rrraitrY2ysrJ662VlZbFq1aoGj3nttddi9uzZUVtbGw899FBMmDAhpk6dGldccUWjY9QaBwAA5NX48eOjoqKi3lppaWnezp9Op6N79+5x8803R6tWrWL//fePN998M6655pqorKxs1DkkQgAAUASa07CE0tLSRic+3bp1i1atWsXq1avrra9evTp69OjR4DE9e/aMNm3aRKtWrerW9txzz1i1alVs3Lgx2rZtu9nrao0DAAAKpm3btrH//vvHvHnz6tbS6XTMmzcvBg0a1OAxBx98cPzlL3+JdPr/sr9XXnklevbs2agkKEIiBAAAFFhFRUXMmDEjbrvttli6dGmMGTMmNmzYUDdFbtSoUTF+/Pi6/ceMGRPvvPNOnHPOOfHKK6/Egw8+GD/84Q/je9/7XqOvqTUOAACKQCadKnQIORs5cmRUV1fHxIkTY9WqVbHvvvvGnDlz6gYorFixIkpK/q+Gs/POO8cjjzwS3//+92OfffaJnXbaKc4555y46KKLGn1NiRAAAFBwY8eOjbFjxzb42oIFCzZZGzRoUDz99NM5X09rHAAAkDgqQgAAUASa09S4lkBFCAAASBwVIQAAKAKZTMsdllAIKkIAAEDiSIQAAIDE0RoHAABFwLCE7KgIAQAAiSMRAgAAEqfRrXHr1q2Lzp071/35s/xzPwAAYOvIpE2Ny0ajE6GuXbtGVVVVdO/ePbbddttIpTa90ZlMJlKpVNTW1uY1SAAAgHxqdCL03//937HddttFRMT8+fObLCAAACB7mUyhI2hZGp0IHXbYYQ3+GQAAoKVpdCL0pz/9qdEn3WeffXIKBgAAYGtodCK07777RiqVisxmam4+IwQAAFufYQnZaXQitHz58qaMAwAAYKtpdCK06667NmUcAAAAW02jE6F/9+qrr8b8+fNjzZo1kU6n6702ceLELQ4MAABoPK1x2ckpEZoxY0aMGTMmunXrFj169Kj3TKFUKrXZRKimpiZqamrqrX2cqY3WqVa5hAMAAJCVnBKhK664Iq688sq46KKLcrrolClTYvLkyfXWDuryhTh4271zOh8AACSd5whlpySXg/7xj3/ECSeckPNFx48fH++++269bWCXvXI+HwAAQDZySoROOOGEmDt3bs4XLS0tjc6dO9fbtMUBAABbS06tcb169YoJEybE008/HXvvvXe0adOm3utnn312XoIDAAAax7CE7OSUCN18882xzTbbxOOPPx6PP/54vddSqZRECAAAaNZySoQ8XBUAAGjJckqEKioqGr3vtGnTcrkEAACQhUxGa1w2ckqEnn/++Vi8eHF8/PHH0bt374iIeOWVV6JVq1ax33771e33r88XAgAAaC5ySoSGDRsWnTp1ittuuy26du0aEZ+M1B49enQceuihcd555+U1SAAAgHzKKRGaOnVqzJ07ty4Jiojo2rVrXHHFFTFkyBCJEAAAbGWZdKEjaFlyeo7QunXrorq6epP16urqWL9+/RYHBQAA0JRyqggde+yxMXr06Jg6dWoccMABERHxzDPPxAUXXBAjRozIa4AAAMDmpQ1LyEpOidD06dPj/PPPj5NPPjk++uijT07UunWcdtppcc011+Q1QAAAgHzLKRHq0KFD3HDDDXHNNdfEX//614iI+NznPhcdO3bMa3AAAABNIadE6J86duwY++yzT75iAQAAcuQ5QtnJaVgCAABASyYRAgAAEmeLWuMAAIDmIZPWGpcNFSEAACBxVIQAAKAIZDKFjqBlURECAAASRyIEAAAkjtY4AAAoAoYlZEdFCAAASByJEAAAkDha4wAAoAikM1rjsqEiBAAAJI5ECAAASBytcQAAUAQyWuOyoiIEAAAkjooQAAAUgUym0BG0LCpCAABA4kiEAACAxNEaBwAARcBzhLKjIgQAACSORAgAAEgcrXEAAFAEPEcoOypCAABA4qgIAQBAEfAcoeyoCAEAAIkjEQIAABJHaxwAABQBzxHKjooQAACQOBIhAAAgcZpNa9zS9PpCh1B0KtPN5l9v0eh718hCh1B0Rux3dqFDKEr3LL6u0CEUnfY7HlroEIqO/0/l3+Tw+1SSeY5QdlSEAACAxPFWDAAAFAHDErKjIgQAACSORAgAAEgcrXEAAFAEMoUOoIVREQIAABJHIgQAACSO1jgAACgCpsZlR0UIAABIHIkQAACQOFrjAACgCGS0xmVFRQgAAEgcFSEAACgC6UIH0MKoCAEAAIkjEQIAABJHaxwAABSBTBiWkA0VIQAAIHEkQgAAQOJojQMAgCKQzhQ6gpZFRQgAAEgcFSEAACgCacMSsqIiBAAAJI5ECAAASBytcQAAUAQ8Ryg7KkIAAEDiSIQAAIDE0RoHAABFIF3oAFoYFSEAACBxJEIAAEDiaI0DAIAiYGpcdlSEAACAxFERAgCAImBYQnZyToSWLVsWP/vZz2Lp0qUREbHnnnvGuHHjonfv3nkLDgAAoCnk1Bp39913R9++fWPRokXRr1+/6NevXyxevDj69u0bd999d75jBAAAyKucKkIXXnhhjB8/Pi677LJ665WVlXHhhRfGcccdl5fgAACAxtEal52cKkJVVVUxatSoTda/8Y1vRFVV1RYHBQAA0JRySoQOP/zweOKJJzZZf/LJJ+PQQw/d4qAAAACaUk6tccccc0xcdNFFsWjRojjwwAMjIuLpp5+Ou+66KyZPnhz3339/vX0BAICm5TlC2ckpETrrrLMiIuKGG26IG264ocHXIiJSqVTU1tZuQXgAAAD5l1MilE77KBYAADQnaQWhrOT0GSEAAICWLKeK0HXXXdfofc8+++xN1mpqaqKmpqbeWm2mNlqlWuUSDgAAQFZySoR+8pOfRHV1dbz//vux7bbbRkTE2rVro0OHDrHDDjvU7ZdKpRpMhKZMmRKTJ0+ut7ZH5z2id5fP5xIOAAAkXtqwhKzk1Bp35ZVXxr777htLly6Nd955J955551YunRp7LfffnHFFVfE8uXLY/ny5fHaa681ePz48ePj3Xffrbf16vy5LfpGAAAAGiunitCECRNi9uzZ0bt377q13r17x09+8pM4/vjj45RTTvnM40tLS6O0tLTemrY4AABga8kpEaqqqoqPP/54k/Xa2tpYvXr1FgcFAABkJ1PoAFqYnFrjjjrqqPjud78bixcvrltbtGhRjBkzJgYPHpy34AAAAJpCTonQzJkzo0ePHjFgwIC6NrcvfvGLUVZWFjNmzMh3jAAAAHmVU2vcDjvsEA899FC8+uqrsXTp0oiI6NOnT3z+86a+AQBAIaQLHUALk1MiVFFRscnaggULGtx32rRpuVwCAACgyeSUCD3//POxePHi+Pjjj+smx73yyivRqlWr2G+//er2S6XMMgcAgK0h7XfvrOSUCA0bNiw6deoUt912W3Tt2jUiIv7xj3/E6NGj49BDD43zzjsvr0ECAADkU07DEqZOnRpTpkypS4IiIrp27RpXXHFFTJ06NW/BAQAANIWcKkLr1q2L6urqTdarq6tj/fr1WxwUAACQHc8Ryk5OFaFjjz02Ro8eHffcc0/8/e9/j7///e9x9913x2mnnRYjRozId4wAAAB5lVNFaPr06XH++efHySefHB999NEnJ2rdOk477bS45ppr8hogAABAvuWUCHXo0CFuuOGGuOaaa+Kvf/1rRER87nOfi44dO+Y1OAAAoHE8Ryg7OSVC/9SxY8fYZ5998hULAADAVrFFiRAAANA8pD1GKCs5DUsAAABoySRCAABA4miNAwCAIpAOvXHZUBECAAASRyIEAAAkjkQIAACKQKYZbbm4/vrro7y8PNq1axcDBw6MZ599tlHH3XHHHZFKpWL48OFZXU8iBAAAFNSsWbOioqIiKisrY/HixdGvX78YOnRorFmz5jOPe/311+P888+PQw89NOtrSoQAAKAIpFPNZ8vWtGnT4owzzojRo0fHXnvtFdOnT48OHTrEzJkzP/WY2traOOWUU2Ly5Mmx++67Z31NiRAAAJBXNTU1sW7dunpbTU1Ng/tu3LgxFi1aFIMHD65bKykpicGDB8fChQs/9RqXXXZZdO/ePU477bScYpQIAQAAeTVlypTo0qVLvW3KlCkN7vvWW29FbW1tlJWV1VsvKyuLVatWNXjMk08+Gb/85S9jxowZOcfoOUIAAFAE0oUO4F+MHz8+Kioq6q2Vlpbm5dzr16+Pb37zmzFjxozo1q1bzueRCAEAAHlVWlra6MSnW7du0apVq1i9enW99dWrV0ePHj022f+vf/1rvP766zFs2LC6tXT6kzSwdevWsWzZsvjc5z632etqjQMAAAqmbdu2sf/++8e8efPq1tLpdMybNy8GDRq0yf59+vSJF198MZYsWVK3HXPMMXHEEUfEkiVLYuedd27UdVWEAACgCOT6/J7moKKiIk499dQYMGBAHHDAAXHttdfGhg0bYvTo0RERMWrUqNhpp51iypQp0a5du+jbt2+947fddtuIiE3WP4tECAAAKKiRI0dGdXV1TJw4MVatWhX77rtvzJkzp26AwooVK6KkJL/NbBIhAACg4MaOHRtjx45t8LUFCxZ85rG33npr1teTCAEAQBHI5UGmSWZYAgAAkDgqQgAAUASa03OEWgIVIQAAIHEkQgAAQOJojQMAgCKgNS47KkIAAEDiSIQAAIDE0RoHAABFIOM5QllREQIAABKn2VSEHl71fKFDKDoPFzqAYnTYHwsdATRK+x0PLXQIReeDlU8UOoSi4+8p5JdhCdlREQIAABJHIgQAACROs2mNAwAAcqc1LjsqQgAAQOJIhAAAgMTRGgcAAEUgU+gAWhgVIQAAIHEkQgAAQOJojQMAgCKQThU6gpZFRQgAAEgcFSEAACgCniOUHRUhAAAgcSRCAABA4miNAwCAIqA1LjsqQgAAQOJIhAAAgMTRGgcAAEUgU+gAWhgVIQAAIHFUhAAAoAikU4WOoGVREQIAABJHIgQAACSO1jgAACgCniOUHRUhAAAgcSRCAABA4miNAwCAIuA5QtlREQIAABJHRQgAAIpAWk0oKypCAABA4kiEAACAxNEaBwAARcBzhLKjIgQAACSORAgAAEgcrXEAAFAEzIzLjooQAACQOBIhAAAgcbTGAQBAETA1LjtbXBH68MMP8xEHAADAVpNTIpROp+Pyyy+PnXbaKbbZZpt47bXXIiJiwoQJ8ctf/jKvAQIAAJuXTjWfrSXIKRG64oor4tZbb42rr7462rZtW7fet2/f+MUvfpG34AAAAJpCTonQ7bffHjfffHOccsop0apVq7r1fv36xcsvv5y34AAAAJpCTsMS3nzzzejVq9cm6+l0Oj766KMtDgoAAMhO2pOEspJTRWivvfaKJ554YpP12bNnR//+/bc4KAAAgKaUU0Vo4sSJceqpp8abb74Z6XQ67rnnnli2bFncfvvt8cADD2z2+Jqamqipqam3lslkIpVqIZ+sAgAAWrScKkJf+9rX4ne/+1089thj0bFjx5g4cWIsXbo0fve738V//ud/bvb4KVOmRJcuXeptmfT6XEIBAAAiItOMtpYglclktnqsDVWEum7fR0UIgGbrg5WbtoSzZdrveGihQ4BG+Xjjm4UOoVEuKT+50CHUufL1Xxc6hM3KqSK0++67x9tvv73J+tq1a2P33Xff7PGlpaXRuXPnepskCAAAcpduRltLkFMi9Prrr0dtbe0m6zU1NfHmmy0jYwYAAJIrq2EJ999/f92fH3nkkejSpUvd17W1tTFv3rwoLy/PW3AAAABNIatEaPjw4RERkUql4tRTT633Wps2baK8vDymTp2at+AAAIDG8Ryh7GSVCKXT6di4cWO0a9cunnnmmfjiF7/YVHEBAAA0maw/I9S2bdvYfvvtY9ttt22CcAAAAJpeTsMSvvGNb8Qvf/nLfMcCAADkqNDPDmppzxHKqjXunz7++OOYOXNmPPbYY7H//vtHx44d670+bdq0vAQHAADQFHJKhF566aXYb7/9IiLilVdeqfea5wEBAADNXU6J0Pz58/MdBwAAsAVayoNMm4ucPiMEAADQkuVUEQIAAJoXzxHKjooQAACQOBIhAAAgcbTGAQBAEdAYlx0VIQAAIHEkQgAAQOJojQMAgCLgOULZURECAAASR0UIAACKQMa4hKyoCAEAAIkjEQIAABJHaxwAABQBwxKyoyIEAAAkjkQIAABIHK1xAABQBNKmxmVFRQgAAEgcFSEAACgC6kHZURECAAASRyIEAAAkjtY4AAAoAoYlZEdFCAAASByJEAAAkDha4wAAoAikCx1AC6MiBAAAJI5ECAAASBytcQAAUAQypsZlRUUIAABIHBUhAAAoAoYlZEdFCAAASByJEAAAkDjNpjXuzYP2KHQIRafjfp0LHULReflO7x3k2+SSjwsdQlGqTDeb/7wXjfY7HlroEIrOByufKHQIReed479d6BAoIMMSsuO3OgAAIHEkQgAAQOLonQAAgCJgalx2VIQAAIDEURECAIAikM4YlpANFSEAACBxJEIAAEDiaI0DAIAioDEuOypCAABA4kiEAACAxNEaBwAARSCtOS4rKkIAAEDiSIQAAIDE0RoHAABFIKM1LisqQgAAQOKoCAEAQBFIFzqAFkZFCAAASByJEAAAkDha4wAAoAh4jlB2VIQAAIDEkQgBAACJozUOAACKgOcIZUdFCAAASBwVIQAAKAKeI5QdFSEAACBxJEIAAEDiaI0DAIAikMkYlpANFSEAACBxJEIAAEDiSIQAAKAIpCPTbLZcXH/99VFeXh7t2rWLgQMHxrPPPvup+86YMSMOPfTQ6Nq1a3Tt2jUGDx78mfs3RCIEAAAU1KxZs6KioiIqKytj8eLF0a9fvxg6dGisWbOmwf0XLFgQJ510UsyfPz8WLlwYO++8cwwZMiTefPPNRl8zp0QonW54Snk6nY4VK1bkckoAACChpk2bFmeccUaMHj069tprr5g+fXp06NAhZs6c2eD+v/rVr+Kss86KfffdN/r06RO/+MUvIp1Ox7x58xp9zawSoXXr1sWJJ54YHTt2jLKyspg4cWLU1tbWvV5dXR277bZbNqcEAADyIN2Mtmxs3LgxFi1aFIMHD65bKykpicGDB8fChQsbdY73338/Pvroo9huu+0afd2sxmdPmDAhXnjhhfiv//qvWLt2bVxxxRWxePHiuOeee6Jt27YRYWwfAAAkXU1NTdTU1NRbKy0tjdLS0k32feutt6K2tjbKysrqrZeVlcXLL7/cqOtddNFFseOOO9ZLpjYnq4rQfffdFzfddFMcf/zxcfrpp8dzzz0X1dXVMWzYsLpvNJVKZXNKAAAgDzLN6J8pU6ZEly5d6m1Tpkxpku/7Rz/6Udxxxx1x7733Rrt27Rp9XFaJUHV1dey66651X3fr1i0ee+yxWL9+fXz5y1+O999/P5vTAQAARWj8+PHx7rvv1tvGjx/f4L7dunWLVq1axerVq+utr169Onr06PGZ1/nxj38cP/rRj2Lu3Lmxzz77ZBVjVonQLrvsEkuXLq231qlTp5g7d2588MEHceyxx2Z1cQAAoPiUlpZG586d620NtcVFRLRt2zb233//eoMO/jn4YNCgQZ96jauvvjouv/zymDNnTgwYMCDrGLNKhIYMGRK33HLLJuvbbLNNPPLII1mVogAAgPwp9LODtuQ5QhUVFTFjxoy47bbbYunSpTFmzJjYsGFDjB49OiIiRo0aVa+idNVVV8WECRNi5syZUV5eHqtWrYpVq1bFe++91+hrZjUsYfLkybFy5coGX+vUqVM8+uijsXjx4mxOCQAAJNzIkSOjuro6Jk6cGKtWrYp999035syZUzdAYcWKFVFS8n81nBtvvDE2btwYxx9/fL3zVFZWxqRJkxp1zawSoX8+ufX222+PkSNHblLeKi0tjTfeeCObUwIAAMTYsWNj7NixDb62YMGCel+//vrrW3y9nB6oOnr06Hj33Xc3WV+/fn1d+QoAANh6MplMs9lagqwqQv+UyWQaHJP997//Pbp06bLZ4xuaK16TTkdpSU55GQAAQFaySoT69+8fqVQqUqlUHHXUUdG69f8dXltbG8uXL4+jjz56s+eZMmVKTJ48ud7aebvuGheUl2cTDgAA8L/ShQ6ghckqERo+fHhERCxZsiSGDh0a22yzTd1rbdu2jfLy8jjuuOM2e57x48dHRUVFvbW1X/1qNqEAAADkLKtEqLKyMiIiysvL4+tf//qnzgLfnNLS0k2O/UBbHAAAsJXklH0ceeSRUV1dXff1s88+G+eee27cfPPNeQsMAABovEwz+qclyCkROvnkk2P+/PkREbFq1aoYPHhwPPvss3HJJZfEZZddltcAAQAA8i2nROill16KAw44ICIi7rzzzth7773jqaeeil/96ldx66235jM+AACAvMtpfPZHH31U9xmfxx57LI455piIiOjTp09UVVXlLzoAAKBR0i2kJa25yKki9IUvfCGmT58eTzzxRDz66KN1I7NXrlwZ22+/fV4DBAAAyLecEqGrrroqbrrppjj88MPjpJNOin79+kVExP3331/XMgcAAGw9mUym2WwtQU6tcQMHDoy///3vsXHjxujatWu88cYbce+998auu+4a3//+9/MdIwAAQF7llAh97WtfixEjRsSZZ54Za9eujQMOOCDatm0bb731VqTT6RgzZky+4wQAAMibnFrjFi9eHIceemhERMyePTt69OgRb7zxRtx+++1x3XXX5TVAAABg89KRaTZbS5BTIvT+++9Hp06dIiJi7ty5MWLEiCgpKYkDDzww3njjjbwGCAAAkG85JUK9evWK++67L/72t7/FI488EkOGDImIiDVr1kTnzp3zGiAAAEC+5ZQITZw4Mc4///woLy+PgQMHxqBBgyLik+pQ//798xogAACweZlm9E9LkNOwhOOPPz4OOeSQqKqqqhudHRFx1FFHxbHHHpu34AAAAJpCTolQRESPHj2iR48e9dY8QwgAAGgJck6EAACA5iPdQh5k2lzk9BkhAACAlkxFCAAAioB6UHZUhAAAgMSRCAEAAImjNQ4AAIpAWnNcVlSEAACAxJEIAQAAiaM1DgAAioDWuOyoCAEAAImjIgQAAEUgk1ERyoaKEAAAkDgSIQAAIHG0xgEAQBEwLCE7KkIAAEDiSIQAAIDE0RoHAABFIKM1LisqQgAAQOJIhAAAgMTRGgcAAEXAA1WzoyIEAAAkjooQAAAUAc8Ryo6KEAAAkDgSIQAAIHG0xgEAQBEwLCE7KkIAAEDiSIQAAIDEaTatcdvNnlnoEIrOb/pNLHQIReelNrWFDqHo7FnoAIrU5Fhf6BBgs945/tuFDqHo+H0q2UyNy46KEAAAkDjNpiIEAADkLqMilBUVIQAAIHEkQgAAQOJojQMAgCKQ9hyhrKgIAQAAiSMRAgAAEkdrHAAAFAFT47KjIgQAACSOihAAABQBwxKyoyIEAAAkjkQIAABIHK1xAABQBAxLyI6KEAAAkDgSIQAAIHG0xgEAQBEwNS47KkIAAEDiSIQAAIDE0RoHAABFwNS47KgIAQAAiaMiBAAARcCwhOyoCAEAAIkjEQIAABJHaxwAABQBwxKyoyIEAAAkjkQIAABIHK1xAABQBDKZdKFDaFFUhAAAgMRREQIAgCKQNiwhKypCAABA4kiEAACAxMm5NW7ZsmXxs5/9LJYuXRoREXvuuWeMGzcuevfunbfgAACAxslktMZlI6eK0N133x19+/aNRYsWRb9+/aJfv36xePHi6Nu3b9x99935jhEAACCvcqoIXXjhhTF+/Pi47LLL6q1XVlbGhRdeGMcdd1xeggMAAGgKOVWEqqqqYtSoUZusf+Mb34iqqqotDgoAAMhOOjLNZmsJckqEDj/88HjiiSc2WX/yySfj0EMP3eKgAAAAmlKjW+Puv//+uj8fc8wxcdFFF8WiRYviwAMPjIiIp59+Ou66666YPHly/qMEAADIo1SmkeMlSkoaVzxKpVJRW1ubdSAfvfVa1sfw2X7Tb2KhQyg6L7XJ/u82FMLS9PpCh1B0Hl71fKFDKDpvHrRHoUMoOtvNnlnoEIpSm267FzqERtmp6xcKHUKdN//xP4UOYbMaXRFKp9NNGQcAAMBWk/NzhP7d2rVrY9ttt83X6QAAgCykPUcoKzkNS7jqqqti1qxZdV+fcMIJsd1228VOO+0UL7zwQt6CAwAAaAo5JULTp0+PnXfeOSIiHn300Xjsscdizpw58aUvfSkuuOCCzR5fU1MT69atq7fV1NTkEgoAAEDWckqEVq1aVZcIPfDAA3HiiSfGkCFD4sILL4w//vGPmz1+ypQp0aVLl3rbVT+dnksoAABARGSa0T8tQU6JUNeuXeNvf/tbRETMmTMnBg8eHBERmUymURPjxo8fH++++2697aJzzswlFAAAgKzlNCxhxIgRcfLJJ8cee+wRb7/9dnzpS1+KiIjnn38+evXqtdnjS0tLo7S0tN7aRxvfyiUUAACArOWUCP3kJz+J8vLy+Nvf/hZXX311bLPNNhERUVVVFWeddVZeAwQAADavkY8H5X/llAgtXLgwzj333Gjduv7h48aNi6eeeiovgQEAADSVnBKhI444IqqqqqJ79+711t9999044ogjGvU5IQAAIH/SLWRIQXOR07CETCYTqVRqk/W33347OnbsuMVBAQAANKWsKkIjRoyIiIhUKhXf+ta36g08qK2tjT/96U9x0EEH5TdCAACAPMsqEerSpUtEfFIR6tSpU7Rv377utbZt28aBBx4YZ5xxRn4jBAAANsuwhOxklQjdcsstERGxww47xKRJk6JDhw4REfH666/HfffdF3vuuWd069Yt/1ECAADkUU6fEXr++efj9ttvj4iItWvXxoEHHhhTp06N4cOHx4033pjXAAEAAPIt50To0EMPjYiI2bNnR1lZWbzxxhtx++23x3XXXZfXAAEAgM1LZzLNZmsJckqE3n///ejUqVNERMydOzdGjBgRJSUlceCBB8Ybb7yR1wABAADyLadEqFevXnHffffF3/72t3jkkUdiyJAhERGxZs2a6Ny5c14DBAAAyLecEqGJEyfG+eefH+Xl5TFw4MAYNGhQRHxSHerfv39eAwQAADYvk8k0m60lyGpq3D8df/zxccghh0RVVVX069evbv2oo46KY489Nm/BAQAANIWcEqGIiB49ekSPHj3qrR1wwAFbHBAAAJC9dLSMSkxzkVNrHAAAQEsmEQIAABIn59Y4AACg+WgpQwqaCxUhAAAgcSRCAABA4miNAwCAIpDWGpcVFSEAACBxVIQAAKAIZDxHKCsqQgAAQOJIhAAAgMTRGgcAAEXAsITsqAgBAACJIxECAAASR2scAAAUgYzWuKyoCAEAAImjIgQAAEXAc4SyoyIEAAAkjkQIAABIHK1xAABQBAxLyI6KEAAAkDgSIQAAIHG0xgEAQBHQGpcdFSEAAKDgrr/++igvL4927drFwIED49lnn/3M/e+6667o06dPtGvXLvbee+946KGHsrqeRAgAACioWbNmRUVFRVRWVsbixYujX79+MXTo0FizZk2D+z/11FNx0kknxWmnnRbPP/98DB8+PIYPHx4vvfRSo68pEQIAgCKQaUZbtqZNmxZnnHFGjB49Ovbaa6+YPn16dOjQIWbOnNng/j/96U/j6KOPjgsuuCD23HPPuPzyy2O//faLn//8542+pkQIAADIq5qamli3bl29raampsF9N27cGIsWLYrBgwfXrZWUlMTgwYNj4cKFDR6zcOHCevtHRAwdOvRT929IsxmW0Kbb7oUOYbNqampiypQpMX78+CgtLS10OJs16s3/V+gQGqWl3deWwD3NP/c0/9zTpuG+5p97mn/uadP4eOObhQ6hzqRJk2Ly5Mn11iorK2PSpEmb7PvWW29FbW1tlJWV1VsvKyuLl19+ucHzr1q1qsH9V61a1egYUxnjJRpt3bp10aVLl3j33Xejc+fOhQ6naLiv+eee5p97mn/uadNwX/PPPc0/97T41dTUbFIBKi0tbTDxXblyZey0007x1FNPxaBBg+rWL7zwwnj88cfjmWee2eSYtm3bxm233RYnnXRS3doNN9wQkydPjtWrVzcqxmZTEQIAAIrDpyU9DenWrVu0atVqkwRm9erV0aNHjwaP6dGjR1b7N8RnhAAAgIJp27Zt7L///jFv3ry6tXQ6HfPmzatXIfpXgwYNqrd/RMSjjz76qfs3REUIAAAoqIqKijj11FNjwIABccABB8S1114bGzZsiNGjR0dExKhRo2KnnXaKKVOmRETEOeecE4cddlhMnTo1vvKVr8Qdd9wRzz33XNx8882NvqZEKAulpaVRWVnpQ3155r7mn3uaf+5p/rmnTcN9zT/3NP/cU/7dyJEjo7q6OiZOnBirVq2KfffdN+bMmVM3EGHFihVRUvJ/zWwHHXRQ/PrXv45LL700fvCDH8Qee+wR9913X/Tt27fR1zQsAQAASByfEQIAABJHIgQAACSORAgAAEgcidAW+Na3vhXDhw8vdBhFb9KkSbHvvvvWfe2+05y8/vrrkUqlYsmSJRERsWDBgkilUrF27dqCxgXQUqVSqbjvvvsKHQYJIBGixfnpT38at956a6HDgAYddNBBUVVVFV26dCl0KCSIXxyb3r+/KRcR8c4778S4ceOid+/e0b59+9hll13i7LPPjnfffbcwQTZD3rykOTM+mxbHL5ibl8lkora2Nlq39iO+tbVt2zarp1rDltq4cWOhQ0islStXxsqVK+PHP/5x7LXXXvHGG2/EmWeeGStXrozZs2cXOjxgMxJfEUqn03H11VdHr169orS0NHbZZZe48sorIyLixRdfjCOPPDLat28f22+/fXznO9+J995771PPNWfOnDjkkENi2223je233z6++tWvxl//+tet9a00C4cffniMGzcuzj333OjatWuUlZXFjBkz6h6I1alTp+jVq1c8/PDDEfF/bUTz5s2LAQMGRIcOHeKggw6KZcuWfeo1kvDuUq738eGHH479998/SktL48knn6x7B/Omm26KnXfeOTp06BAnnnjiJu9Wzpw5M77whS9EaWlp9OzZM8aOHVuIb7sgNmzYEKNGjYptttkmevbsGVOnTo3DDz88zj333IiIKC8vjx/+8Ifx7W9/Ozp16hS77LLLZz6sTWvc5u/pmjVrYtiwYdG+ffvYbbfd4le/+lWUl5fHtddeW3eOVCoVN954Y3zpS1+K9u3bx+677+4Xy/91+OGHx9ixY+Pcc8+Nbt261T2H5dhjj41UKhXl5eV1+/72t7+N/fbbL9q1axe77757TJ48OT7++OMCRd58zJ49O/bee++6/78PHjw4NmzYEAsWLIgDDjggOnbsGNtuu20cfPDB8cYbb8Stt94akydPjhdeeCFSqVSkUqm49dZbo2/fvnH33XfHsGHD4nOf+1wceeSRceWVV8bvfve7xN3nhu7pBRdcELfddlv89re/rbtvCxYsiI0bN8bYsWOjZ8+e0a5du9h1113rHpL5T1VVVX7+aXKJT4TGjx8fP/rRj2LChAnx5z//OX79619HWVlZbNiwIYYOHRpdu3aNP/7xj3HXXXfFY4899pm/IG7YsCEqKiriueeei3nz5kVJSUkce+yxkU6nt+J3VHi33XZbdOvWLZ599tkYN25cjBkzJk444YQ46KCDYvHixTFkyJD45je/Ge+//37dMZdccklMnTo1nnvuuWjdunV8+9vfLuB30Dzkch8vvvji+NGPfhRLly6NffbZJyIi/vKXv8Sdd94Zv/vd72LOnDnx/PPPx1lnnVV3zI033hjf+9734jvf+U68+OKLcf/990evXr22+vdbKBdccEE8/vjj8dvf/jbmzp0bCxYsiMWLF9fbZ+rUqTFgwIC6ezdmzJjPTNaTbnP39Fvf+lb87W9/i/nz58fs2bPjhhtuiDVr1mxyngkTJsRxxx0XL7zwQpxyyinx9a9/PZYuXbo1v5Vm67bbbou2bdvGH/7wh3j66acjIuKWW26Jqqqq+OMf/xgREU888USMGjUqzjnnnPjzn/8cN910U9x66611b/YlVVVVVZx00knx7W9/O5YuXRoLFiyIESNGRCaTieHDh8dhhx0Wf/rTn2LhwoXxne98J1KpVIwcOTLOO++8+MIXvhBVVVVRVVUVI0eObPD87777bnTu3DlRFflPu6eVlZVx4oknxtFHH1133w466KC47rrr4v77748777wzli1bVvdmyL/y889WkUmwdevWZUpLSzMzZszY5LWbb74507Vr18x7771Xt/bggw9mSkpKMqtWrcpkMpnMqaeemvna1772qeevrq7ORETmxRdfzHvszdVhhx2WOeSQQ+q+/vjjjzMdO3bMfPOb36xbq6qqykREZuHChZn58+dnIiLz2GOP1b3+4IMPZiIi88EHH2QymUymsrIy069fv7rXN3ffi0Gu9/G+++6rd57KyspMq1atMn//+9/r1h5++OFMSUlJpqqqKpPJZDI77rhj5pJLLmni76h5Wr9+faZt27aZO++8s27t7bffzrRv3z5zzjnnZDKZTGbXXXfNfOMb36h7PZ1OZ7p375658cYbM5lMJrN8+fJMRGSef/75TCaTqft38Y9//GNrfRvNyubu6bJlyzIRkXn22WfrXl+6dGkmIjI/+clP6tYiInPmmWfWO/fAgQMzY8aMafLvobk77LDDMv3796+3FhGZe++9t97aUUcdlfnhD39Yb+2//uu/Mj179mzqEJu1RYsWZSIi8/rrr9dbf/vttzMRkVmwYEGDx/37/4saUl1dndlll10yP/jBD/IVbovwafc0k2n4/9njxo3LHHnkkZl0Ot3g+fz8s7UkuiK0dOnSqKmpiaOOOqrB1/r16xcdO3asWzv44IMjnU5/6jvBr776apx00kmx++67R+fOneve3VixYkWTxN9c/bMSERHRqlWr2H777WPvvfeuWysrK4uIqPcO8L8e07Nnz01eT6Jc7uOAAQM2Oc8uu+wSO+20U93XgwYNqvt7vGbNmli5cmWDPwNJ8Ne//jU2btwYAwcOrFvbbrvtonfv3vX2+9d/F6lUKnr06JH4v5+fZnP3dOnSpdG6devYf//9617v06dPbLvttpuca9CgQZt87R3hT/zr/fs0L7zwQlx22WWxzTbb1G1nnHFGVFVV1askJ02/fv3iqKOOir333jtOOOGEmDFjRvzjH/+I7bbbLr71rW/F0KFDY9iwYfHTn/40qqqqGn3edevWxVe+8pXYa6+9YtKkSU33DTRDn3ZPP823vvWtWLJkSfTu3TvOPvvsmDt37ib7+Plna0h0ItS+ffu8nm/YsGHxzjvvxIwZM+KZZ56JZ555JiKS90HWNm3a1Ps6lUrVW0ulUhER9VoGN/d6EuVyH/81cW+MfP8MFKuG/l0k/e8nhdWYn/X33nsvJk+eHEuWLKnbXnzxxXj11VejXbt2WyHK5qlVq1bx6KOPxsMPPxx77bVX/OxnP4vevXvH8uXL45ZbbomFCxfGQQcdFLNmzYrPf/7zda2Hn2X9+vVx9NFHR6dOneLee+/d5L8Zxe6z7mlD9ttvv1i+fHlcfvnl8cEHH8SJJ54Yxx9//FaOGhKeCO2xxx7Rvn37mDdv3iav7bnnnvHCCy/Ehg0b6tb+8Ic/RElJySbvFkdEvP3227Fs2bK49NJL46ijjoo999zzM98Nga1lxYoVsXLlyrqvn3766bq/x506dYry8vIGfwaS4HOf+1y0adOm7k2LiIh//OMf8corrxQwqpZtc/e0T58+8fHHH8eiRYvqXl+2bFmDwyX+/RfQp59+Ovbcc8+mCbyFa9OmTdTW1tZb22+//WLZsmXRq1evTbaSkkT/7z9SqVQcfPDBMXny5Hj++eejbdu2ce+990ZERP/+/WP8+PHx1FNPRd++fePXv/51RHwyEfLf73HEJ5WgIUOGRNu2beP+++9PbJL5aff00+5b586dY+TIkTFjxoyYNWtW3H333fHOO+/Uve7nn60hOZ/ka0C7du3ioosuigsvvDDatm0bBx98cFRXV8f//M//xCmnnBKVlZVx6qmnxqRJk6K6ujrGjRsX3/zmN+takv5V165dY/vtt4+bb745evbsGStWrIiLL764AN8V1NeuXbs49dRT48c//nGsW7cuzj777DjxxBPrRjxPmjQpzjzzzOjevXt86UtfivXr18cf/vCHGDduXIEjb3rbbLNNnHbaaXHBBRfE9ttvH927d49LLrkk8b8kbonN3dPevXvH0UcfHd/97nfjxhtvjNatW8e5557bYHXyrrvuigEDBsQhhxwSv/rVr+LZZ5+NX/7yl1v7W2oR/vmGxsEHHxylpaXRtWvXmDhxYnz1q1+NXXbZJY4//vgoKSmJF154IV566aW44oorCh1ywTzzzDMxb968GDJkSHTv3j2eeeaZqK6ujvbt28f48ePjmGOOiR133DGWLVsWr776aowaNSoiPrnHy5cvjyVLlsR//Md/RKdOnaKmpiaGDBkS77//fvy///f/Yt26dbFu3bqIiNhhhx2iVatWhfxWt5pPu6d77rlnfPjhh/HII4/EsmXLYvvtt48uXbrEz372s+jZs2f0798/SkpK4q677ooePXrUa5H188/WkOhEKOKTqSStW7eOiRMnxsqVK6Nnz55x5plnRocOHeKRRx6Jc845J774xS9Ghw4d4rjjjotp06Y1eJ6SkpK444474uyzz46+fftG796947rrrovDDz98635D8G969eoVI0aMiC9/+cvxzjvvxFe/+tW44YYb6l4/9dRT48MPP4yf/OQncf7550e3bt0S1aJwzTXXxHvvvRfDhg2LTp06xXnnnedhiFtoc/f0lltuidNPPz0OO+ywKCsriyuuuCImTJiwyXkmT54cd9xxR5x11lnRs2fP+M1vfhN77bXX1vxWWoypU6dGRUVFzJgxI3baaad4/fXXY+jQofHAAw/EZZddFldddVW0adMm+vTpE6effnqhwy2ozp07x+9///u49tprY926dbHrrrvG1KlTY8SIEXHmmWfGbbfdFm+//Xb07Nkzvve978V3v/vdiIg47rjj4p577okjjjgi1q5dG7fcckuUl5fXVT//fdrm8uXLN5mEVqw+7Z5+6UtfigEDBsSCBQtiwIAB8d5778X8+fOjU6dOcfXVV8err74arVq1ii9+8Yvx0EMP1XsTys8/W0Mqk8lkCh0E0DQmTZoU9913XyxZsqTQobQohx9+eOy77771nmvDltncPS0vL49zzz237llDqVQq7r333qJ/ZhgAhaP/AwAASByJEAAAkDha4wAAgMRREQIAABJHIgQAACSORAgAAEgciRAAAJA4EiEAACBxJEIAAEDiSIQAAIDEkQgBAACJIxECAAAS5/8DXKkS4oBuOFYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1100x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(df_all[10])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Obtaining dependency information for seaborn from https://files.pythonhosted.org/packages/83/11/00d3c3dfc25ad54e731d91449895a79e4bf2384dc3ac01809010ba88f6d5/seaborn-0.13.2-py3-none-any.whl.metadata\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in ./ShkodnikVenv/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in ./ShkodnikVenv/lib/python3.11/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./ShkodnikVenv/lib/python3.11/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./ShkodnikVenv/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./ShkodnikVenv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./ShkodnikVenv/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./ShkodnikVenv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, WeightLoraConfig\n",
    "target_modules=['up_proj', 'down_proj', 'gate_proj', \n",
    "                    'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "peft_config = WeightLoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightLoraConfig'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name: str = \"/media/ssd-3t/akazakov/llama31instr/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/07eb05b21d191a58c577b4a45982fe0c049d0693\"\n",
    "    # model_name: str = \"unsloth/Meta-Llama-3.1-8B\" \n",
    "    #model_name: str = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "    model_name: str = \"FacebookAI/roberta-base\"\n",
    "    max_seq_length: int = 1000\n",
    "    dtype: str = None\n",
    "    load_in_4bit: bool = False\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(TrainingArguments):\n",
    "    per_device_train_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    warmup_steps: int = 5\n",
    "    num_train_epochs: int = 5\n",
    "    learning_rate: float = 1e-10\n",
    "    logging_steps: int = 1\n",
    "    optim: str = \"adamw_hf\"\n",
    "    weight_decay: float = 0.01\n",
    "    lr_scheduler_type: str = \"linear\"\n",
    "    seed: int = 18\n",
    "    output_dir: str = \"train_outputs\"\n",
    "    # output_dir: str = None\n",
    "    sign_step: int = 5000\n",
    "    max_grad_norm: float = 1.0\n",
    "    max_steps: int = 2 # overrides num_train_epochs\n",
    "    report_to: str = \"none\" # \"none\" or \"wandb\"\n",
    " \n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    train_file: str = 'data/train_ft_short_system.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipelines.adapters as adapters\n",
    "import pipelines.optimizers as optimizers\n",
    "import pipelines.utils as utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArguments\n",
    "training_args = TrainingArguments\n",
    "utils.set_seed(18)\n",
    "device = utils.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name,\n",
    "    load_in_8bit=model_args.load_in_4bit,\n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 125,139,801 || trainable%: 0.3535\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): RobertaForCausalLM(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): RobertaLMHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import WeightLoraConfig, LoraConfig, LoKrConfig\n",
    "from peft import LoKrModel, LoraModel, WeightLoraModel\n",
    "from peft import get_peft_model\n",
    "from peft import PeftConfig, PeftType\n",
    "target_modules = [\"query\", \"key\", \"value\"]\n",
    "weight_lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=target_modules,\n",
    "    # rank_dropout=0.0,\n",
    "    # module_dropout=0.0,\n",
    ")\n",
    "# model = WeightLoraModel(model, weight_lora_config, adapter_name=\"default\")\n",
    "# model = LoraModel(model, weight_lora_config, adapter_name=\"default\")\n",
    "model = get_peft_model(model, weight_lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 34/34 [00:00<00:00, 1837.36 examples/s]\n",
      "Map: 100%|| 311/311 [00:00<00:00, 10784.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "        num_rows: 34\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset_name = 'cais/mmlu'\n",
    "dataset_config_name = 'philosophy'\n",
    "dataset = datasets.load_dataset(dataset_name, dataset_config_name)\n",
    "train = utils.make_mlm_dataset_form_mmlu(dataset['test'])\n",
    "test = utils.make_mlm_dataset_form_mmlu(dataset['validation'])\n",
    "dataset = datasets.DatasetDict({\"test\" : test, \"train\" : train})\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if \"lm_head\" in name or \"embed\" in name:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7f8760328140>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of pipelines.optimizers failed: Traceback (most recent call last):\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: step() requires a code object with 1 free vars, not 2\n",
      "]\n",
      "/home/shkodnik/Sber_Lora/pipelines/optimizers.py:164: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "optimizer = optimizers.AdamW(model.parameters(), \n",
    "                                 lr=training_args.learning_rate,\n",
    "                                 weight_decay=training_args.weight_decay)\n",
    "scheduler = get_scheduler(name=training_args.lr_scheduler_type, \n",
    "                            optimizer=optimizer, \n",
    "                            num_warmup_steps=training_args.warmup_steps,\n",
    "                            num_training_steps=training_args.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/peft/utils/other.py:629: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b6dd6655-0be4-421f-963d-54d210a62472)') - silently ignoring the lookup for the file config.json in FacebookAI/roberta-base.\n",
      "  warnings.warn(\n",
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in FacebookAI/roberta-base - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m            . \n",
      "\u001b[1;31m   ,     . \n",
      "\u001b[1;31m <a href='https://aka.ms/vscodeJupyterKernelCrash'></a>,    . \n",
      "\u001b[1;31m .  <a href='command:jupyter.viewOutput'> Jupyter</a>."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "run_name = \"test\"\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #train_dataset=dataset,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    args=TrainingArguments(\n",
    "        max_steps=10,\n",
    "        # learning_rate=1e-3, \n",
    "        # fp16=True, \n",
    "        output_dir=training_args.output_dir, \n",
    "        use_cpu=False, \n",
    "        save_safetensors=False,\n",
    "        # report_to=report_to,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=1,\n",
    "        # run_name=f\"prob={int(prob*100)}/100_k={k}\",\n",
    "        run_name=run_name,\n",
    "        # run_name=\"test\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15),\n",
    "    optimizers=[optimizer, scheduler]\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mSFTTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingArguments\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_collator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataCollator\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizerBase\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainerCallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambdaLR\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PeftConfig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_text_field\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpacking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mformatting_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minfinite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_of_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mchars_per_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_num_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mneftune_noise_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_init_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_packing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Class definition of the Supervised Finetuning Trainer (SFT Trainer).\n",
      "This class is a wrapper around the `transformers.Trainer` class and inherits all of its attributes and methods.\n",
      "The trainer takes care of properly initializing the PeftModel in case a user passes a `PeftConfig` object.\n",
      "\n",
      "Args:\n",
      "    model (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]):\n",
      "        The model to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the model name to\n",
      "        load from cache or download. The model can be also converted to a `PeftModel` if a `PeftConfig` object is\n",
      "        passed to the `peft_config` argument.\n",
      "    args (Optional[`transformers.TrainingArguments`]):\n",
      "        The arguments to tweak for training. Please refer to the official documentation of `transformers.TrainingArguments`\n",
      "        for more information.\n",
      "    data_collator (Optional[`transformers.DataCollator`]):\n",
      "        The data collator to use for training.\n",
      "    train_dataset (Optional[`datasets.Dataset`]):\n",
      "        The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n",
      "    eval_dataset (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]):\n",
      "        The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n",
      "    tokenizer (Optional[`transformers.PreTrainedTokenizer`]):\n",
      "        The tokenizer to use for training. If not specified, the tokenizer associated to the model will be used.\n",
      "    model_init (`Callable[[], transformers.PreTrainedModel]`):\n",
      "        The model initializer to use for training. If None is specified, the default model initializer will be used.\n",
      "    compute_metrics (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to None):\n",
      "        The function used to compute metrics during evaluation. It should return a dictionary mapping metric names to metric values.\n",
      "        If not specified, only the loss will be computed during evaluation.\n",
      "    callbacks (`List[transformers.TrainerCallback]`):\n",
      "        The callbacks to use for training.\n",
      "    optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):\n",
      "        The optimizer and scheduler to use for training.\n",
      "    preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):\n",
      "        The function to use to preprocess the logits before computing the metrics.\n",
      "    peft_config (`Optional[PeftConfig]`):\n",
      "        The PeftConfig object to use to initialize the PeftModel.\n",
      "    dataset_text_field (`Optional[str]`):\n",
      "        The name of the text field of the dataset, in case this is passed by a user, the trainer will automatically create a\n",
      "        `ConstantLengthDataset` based on the `dataset_text_field` argument.\n",
      "    formatting_func (`Optional[Callable]`):\n",
      "        The formatting function to be used for creating the `ConstantLengthDataset`.\n",
      "    max_seq_length (`Optional[int]`):\n",
      "        The maximum sequence length to use for the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults to `512`.\n",
      "    infinite (`Optional[bool]`):\n",
      "        Whether to use an infinite dataset or not. Defaults to `False`.\n",
      "    num_of_sequences (`Optional[int]`):\n",
      "        The number of sequences to use for the `ConstantLengthDataset`. Defaults to `1024`.\n",
      "    chars_per_token (`Optional[float]`):\n",
      "        The number of characters per token to use for the `ConstantLengthDataset`. Defaults to `3.6`. You can check how this is computed in the\n",
      "        stack-llama example: https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53.\n",
      "    packing (`Optional[bool]`):\n",
      "        Used only in case `dataset_text_field` is passed. This argument is used by the `ConstantLengthDataset` to pack the sequences\n",
      "        of the dataset.\n",
      "    dataset_num_proc (`Optional[int]`):\n",
      "        The number of workers to use to tokenize the data. Only used when `packing=False`. Defaults to None.\n",
      "    dataset_batch_size (`int`):\n",
      "        The number of examples to tokenize per batch. If batch_size <= 0 or batch_size == None,\n",
      "        tokenize the full dataset as a single batch. Defaults to 1000.\n",
      "    neftune_noise_alpha (`Optional[float]`):\n",
      "        If not `None`, this will activate NEFTune noise embeddings. This has been proven to drastically improve model performances for instruction\n",
      "        fine-tuning. Check out the original paper here: https://arxiv.org/abs/2310.05914 and the original code here: https://github.com/neelsjain/NEFTune\n",
      "    model_init_kwargs: (`Optional[Dict]`, *optional*):\n",
      "        Dict of Optional kwargs to pass when instantiating the model from a string\n",
      "    dataset_kwargs: (`Optional[Dict]`, *optional*):\n",
      "        Dict of Optional kwargs to pass when creating packed or non-packed datasets\n",
      "    eval_packing: (`Optional[bool]`, *optional*):\n",
      "        Whether to pack the eval dataset as well. Defaults to `packing` if `None` is passed.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     UnslothTrainer"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "?SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight ; sum =  nan\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora_A\" in name:\n",
    "        print(name, \"; sum = \", param.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ShkodnikVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
