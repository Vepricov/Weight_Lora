{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "# from unsloth import FastLanguageModel\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "# from unsloth import is_bfloat16_supported\n",
    "# from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from dataclasses import dataclass\n",
    "from datasets import Dataset\n",
    "import torch_optimizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor()\n",
    "b = torch.Tensor([1, 2])\n",
    "a = torch.cat([a, b])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, WeightLoraConfig\n",
    "target_modules=['up_proj', 'down_proj', 'gate_proj', \n",
    "                    'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "peft_config = WeightLoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightLoraConfig'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    #model_name: str = \"/media/ssd-3t/akazakov/llama31instr/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/07eb05b21d191a58c577b4a45982fe0c049d0693\"\n",
    "    # model_name: str = \"unsloth/Meta-Llama-3.1-8B\" \n",
    "    #model_name: str = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "    model_name: str = \"FacebookAI/roberta-base\"\n",
    "    max_seq_length: int = 1000\n",
    "    dtype: str = None\n",
    "    load_in_4bit: bool = False\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(TrainingArguments):\n",
    "    per_device_train_batch_size: int = 16\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    warmup_steps: int = 5\n",
    "    num_train_epochs: int = 5\n",
    "    learning_rate: float = 1e-10\n",
    "    logging_steps: int = 1\n",
    "    optim: str = \"adamw_hf\"\n",
    "    weight_decay: float = 0.01\n",
    "    lr_scheduler_type: str = \"linear\"\n",
    "    seed: int = 18\n",
    "    output_dir: str = \"train_outputs\"\n",
    "    # output_dir: str = None\n",
    "    sign_step: int = 5000\n",
    "    max_grad_norm: float = 1.0\n",
    "    max_steps: int = 2 # overrides num_train_epochs\n",
    "    report_to: str = \"none\" # \"none\" or \"wandb\"\n",
    " \n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    train_file: str = 'data/train_ft_short_system.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipelines.adapters as adapters\n",
    "import pipelines.optimizers as optimizers\n",
    "import pipelines.utils as utils\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArguments\n",
    "training_args = TrainingArguments\n",
    "utils.set_seed(18)\n",
    "device = utils.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name,\n",
    "    load_in_8bit=model_args.load_in_4bit,\n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 442,368 || all params: 125,139,801 || trainable%: 0.3535\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): RobertaForCausalLM(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): RobertaLMHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import WeightLoraConfig, LoraConfig, LoKrConfig\n",
    "from peft import LoKrModel, LoraModel, WeightLoraModel\n",
    "from peft import get_peft_model\n",
    "from peft import PeftConfig, PeftType\n",
    "target_modules = [\"query\", \"key\", \"value\"]\n",
    "weight_lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=target_modules,\n",
    "    # rank_dropout=0.0,\n",
    "    # module_dropout=0.0,\n",
    ")\n",
    "# model = WeightLoraModel(model, weight_lora_config, adapter_name=\"default\")\n",
    "# model = LoraModel(model, weight_lora_config, adapter_name=\"default\")\n",
    "model = get_peft_model(model, weight_lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34/34 [00:00<00:00, 1837.36 examples/s]\n",
      "Map: 100%|██████████| 311/311 [00:00<00:00, 10784.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "        num_rows: 34\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "        num_rows: 311\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset_name = 'cais/mmlu'\n",
    "dataset_config_name = 'philosophy'\n",
    "dataset = datasets.load_dataset(dataset_name, dataset_config_name)\n",
    "train = utils.make_mlm_dataset_form_mmlu(dataset['test'])\n",
    "test = utils.make_mlm_dataset_form_mmlu(dataset['validation'])\n",
    "dataset = datasets.DatasetDict({\"test\" : test, \"train\" : train})\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], return_special_tokens_mask=True)\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True\n",
    ")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if \"lm_head\" in name or \"embed\" in name:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_parameters at 0x7f8760328140>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of pipelines.optimizers failed: Traceback (most recent call last):\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: step() requires a code object with 1 free vars, not 2\n",
      "]\n",
      "/home/shkodnik/Sber_Lora/pipelines/optimizers.py:164: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "optimizer = optimizers.AdamW(model.parameters(), \n",
    "                                 lr=training_args.learning_rate,\n",
    "                                 weight_decay=training_args.weight_decay)\n",
    "scheduler = get_scheduler(name=training_args.lr_scheduler_type, \n",
    "                            optimizer=optimizer, \n",
    "                            num_warmup_steps=training_args.warmup_steps,\n",
    "                            num_training_steps=training_args.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.825100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n",
      "274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/peft/utils/other.py:629: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b6dd6655-0be4-421f-963d-54d210a62472)') - silently ignoring the lookup for the file config.json in FacebookAI/roberta-base.\n",
      "  warnings.warn(\n",
      "/home/shkodnik/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in FacebookAI/roberta-base - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "run_name = \"test\"\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    #train_dataset=dataset,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    args=TrainingArguments(\n",
    "        max_steps=10,\n",
    "        # learning_rate=1e-3, \n",
    "        # fp16=True, \n",
    "        output_dir=training_args.output_dir, \n",
    "        use_cpu=False, \n",
    "        save_safetensors=False,\n",
    "        # report_to=report_to,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=1,\n",
    "        # run_name=f\"prob={int(prob*100)}/100_k={k}\",\n",
    "        run_name=run_name,\n",
    "        # run_name=\"test\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15),\n",
    "    optimizers=[optimizer, scheduler]\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mSFTTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainingArguments\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_collator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataCollator\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_dataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizerBase\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainerCallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambdaLR\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpeft_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PeftConfig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_text_field\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpacking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mformatting_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minfinite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_of_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mchars_per_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_num_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mneftune_noise_alpha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel_init_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meval_packing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Class definition of the Supervised Finetuning Trainer (SFT Trainer).\n",
      "This class is a wrapper around the `transformers.Trainer` class and inherits all of its attributes and methods.\n",
      "The trainer takes care of properly initializing the PeftModel in case a user passes a `PeftConfig` object.\n",
      "\n",
      "Args:\n",
      "    model (Union[`transformers.PreTrainedModel`, `nn.Module`, `str`]):\n",
      "        The model to train, can be a `PreTrainedModel`, a `torch.nn.Module` or a string with the model name to\n",
      "        load from cache or download. The model can be also converted to a `PeftModel` if a `PeftConfig` object is\n",
      "        passed to the `peft_config` argument.\n",
      "    args (Optional[`transformers.TrainingArguments`]):\n",
      "        The arguments to tweak for training. Please refer to the official documentation of `transformers.TrainingArguments`\n",
      "        for more information.\n",
      "    data_collator (Optional[`transformers.DataCollator`]):\n",
      "        The data collator to use for training.\n",
      "    train_dataset (Optional[`datasets.Dataset`]):\n",
      "        The dataset to use for training. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n",
      "    eval_dataset (Optional[Union[`datasets.Dataset`, Dict[`str`, `datasets.Dataset`]]]):\n",
      "        The dataset to use for evaluation. We recommend users to use `trl.trainer.ConstantLengthDataset` to create their dataset.\n",
      "    tokenizer (Optional[`transformers.PreTrainedTokenizer`]):\n",
      "        The tokenizer to use for training. If not specified, the tokenizer associated to the model will be used.\n",
      "    model_init (`Callable[[], transformers.PreTrainedModel]`):\n",
      "        The model initializer to use for training. If None is specified, the default model initializer will be used.\n",
      "    compute_metrics (`Callable[[transformers.EvalPrediction], Dict]`, *optional* defaults to None):\n",
      "        The function used to compute metrics during evaluation. It should return a dictionary mapping metric names to metric values.\n",
      "        If not specified, only the loss will be computed during evaluation.\n",
      "    callbacks (`List[transformers.TrainerCallback]`):\n",
      "        The callbacks to use for training.\n",
      "    optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`):\n",
      "        The optimizer and scheduler to use for training.\n",
      "    preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`):\n",
      "        The function to use to preprocess the logits before computing the metrics.\n",
      "    peft_config (`Optional[PeftConfig]`):\n",
      "        The PeftConfig object to use to initialize the PeftModel.\n",
      "    dataset_text_field (`Optional[str]`):\n",
      "        The name of the text field of the dataset, in case this is passed by a user, the trainer will automatically create a\n",
      "        `ConstantLengthDataset` based on the `dataset_text_field` argument.\n",
      "    formatting_func (`Optional[Callable]`):\n",
      "        The formatting function to be used for creating the `ConstantLengthDataset`.\n",
      "    max_seq_length (`Optional[int]`):\n",
      "        The maximum sequence length to use for the `ConstantLengthDataset` and for automatically creating the Dataset. Defaults to `512`.\n",
      "    infinite (`Optional[bool]`):\n",
      "        Whether to use an infinite dataset or not. Defaults to `False`.\n",
      "    num_of_sequences (`Optional[int]`):\n",
      "        The number of sequences to use for the `ConstantLengthDataset`. Defaults to `1024`.\n",
      "    chars_per_token (`Optional[float]`):\n",
      "        The number of characters per token to use for the `ConstantLengthDataset`. Defaults to `3.6`. You can check how this is computed in the\n",
      "        stack-llama example: https://github.com/huggingface/trl/blob/08f550674c553c36c51d1027613c29f14f3676a5/examples/stack_llama/scripts/supervised_finetuning.py#L53.\n",
      "    packing (`Optional[bool]`):\n",
      "        Used only in case `dataset_text_field` is passed. This argument is used by the `ConstantLengthDataset` to pack the sequences\n",
      "        of the dataset.\n",
      "    dataset_num_proc (`Optional[int]`):\n",
      "        The number of workers to use to tokenize the data. Only used when `packing=False`. Defaults to None.\n",
      "    dataset_batch_size (`int`):\n",
      "        The number of examples to tokenize per batch. If batch_size <= 0 or batch_size == None,\n",
      "        tokenize the full dataset as a single batch. Defaults to 1000.\n",
      "    neftune_noise_alpha (`Optional[float]`):\n",
      "        If not `None`, this will activate NEFTune noise embeddings. This has been proven to drastically improve model performances for instruction\n",
      "        fine-tuning. Check out the original paper here: https://arxiv.org/abs/2310.05914 and the original code here: https://github.com/neelsjain/NEFTune\n",
      "    model_init_kwargs: (`Optional[Dict]`, *optional*):\n",
      "        Dict of Optional kwargs to pass when instantiating the model from a string\n",
      "    dataset_kwargs: (`Optional[Dict]`, *optional*):\n",
      "        Dict of Optional kwargs to pass when creating packed or non-packed datasets\n",
      "    eval_packing: (`Optional[bool]`, *optional*):\n",
      "        Whether to pack the eval dataset as well. Defaults to `packing` if `None` is passed.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Sber_Lora/ShkodnikVenv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     UnslothTrainer"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "?SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.roberta.encoder.layer.0.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.0.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.1.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.2.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.3.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.4.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.5.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.6.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.7.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.8.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.9.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.10.attention.self.value.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.query.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.key.lora_A.default.weight ; sum =  nan\n",
      "base_model.model.roberta.encoder.layer.11.attention.self.value.lora_A.default.weight ; sum =  nan\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora_A\" in name:\n",
    "        print(name, \"; sum = \", param.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ShkodnikVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
